<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://zatoima.github.io/favicon.ico" />
<title>LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-23ï¼‰ | my opinion is my own</title>
<meta name="title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-23ï¼‰" />
<meta name="description" content="" />
<meta name="keywords" content="LLM,AI,è«–æ–‡," />


<meta property="og:title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-23ï¼‰" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zatoima.github.io/llm-papers-2026-02-23/" /><meta property="og:image" content="https://zatoima.github.io/images/share.png"/><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2026-02-23T00:00:00+00:00" />
<meta property="article:modified_time" content="2026-02-23T00:00:00+00:00" /><meta property="og:site_name" content="my opinion is my own" />




<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://zatoima.github.io/images/share.png"/>

<meta name="twitter:title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-23ï¼‰"/>
<meta name="twitter:description" content=""/>



<meta itemprop="name" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-23ï¼‰">
<meta itemprop="description" content=""><meta itemprop="datePublished" content="2026-02-23T00:00:00+00:00" />
<meta itemprop="dateModified" content="2026-02-23T00:00:00+00:00" />
<meta itemprop="wordCount" content="1102"><meta itemprop="image" content="https://zatoima.github.io/images/share.png"/>
<meta itemprop="keywords" content="LLM,AI,è«–æ–‡," />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <link rel="alternate" type="text/plain" href="/llms.txt" title="LLMs.txt" />
  <link rel="alternate" type="text/plain" href="/llms-full.txt" title="LLMs-full.txt" />

  <script>
    (function(){
      var t = localStorage.getItem('theme');
      if (t === 'dark' || (!t && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
        document.documentElement.setAttribute('data-theme', 'dark');
      }
    })();
  </script><link rel="stylesheet" href="/css/zenn.css">

<script async src="https://www.googletagmanager.com/gtag/js?id=G-STFZ9QMXGM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-STFZ9QMXGM');
</script>
</head>

<body>
  <div class="site-wrapper">
    <header class="site-header">
      <div class="header-inner">
        <a href="/" class="site-logo">my opinion is my own</a>
        <nav class="header-nav">
<a href="/about/">About</a>
<a href="/blog/">Blog</a>
<a href="/index.xml">RSS</a>
<a href="/other/">Other</a>
<a href="/llms.txt" title="LLMs.txt - AI/LLMå‘ã‘ã‚µã‚¤ãƒˆæƒ…å ±">llms.txt</a>
</nav>
        <div class="header-actions">
          <button id="search-btn" class="header-icon-btn" aria-label="æ¤œç´¢">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><path d="m21 21-4.3-4.3"/></svg>
          </button>
          <button id="dark-mode-btn" class="header-icon-btn" aria-label="ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿">ğŸŒ™</button>
          <button id="hamburger-btn" class="hamburger-btn" aria-label="ãƒ¡ãƒ‹ãƒ¥ãƒ¼">
            <span></span><span></span><span></span>
          </button>
        </div>
      </div>
    </header>

    
    <div id="search-overlay" class="search-overlay">
      <div class="search-modal">
        <input type="text" id="search-input" class="search-input" placeholder="è¨˜äº‹ã‚’æ¤œç´¢... (Ctrl+K)" autocomplete="off">
        <div id="search-results" class="search-results"></div>
      </div>
    </div>

    
    <div id="nav-overlay" class="nav-overlay"></div>

    <main class="site-main">
<div class="article-layout">
  <article class="article-main">
    
    <nav class="breadcrumb" aria-label="ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆ">
  <a href="/">ãƒ›ãƒ¼ãƒ </a>
  <span class="breadcrumb-sep">&gt;</span>
  
  <a href="/blog/">Blog</a>
  <span class="breadcrumb-sep">&gt;</span>
  
  <span class="breadcrumb-current">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-23ï¼‰</span>
</nav>

    
    <div class="article-card">
      <div class="article-header">
        <div class="article-emoji">ğŸ“</div>
        <h1>LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-23ï¼‰</h1>
        
        <div class="article-meta">
          <time class="article-date" datetime='2026-02-23'>
            2026/02/23 ã«å…¬é–‹
          </time>
          
          
          
          <span class="reading-time">ğŸ“– ç´„2åˆ†</span>
        </div>
        <div class="article-tags">
          
          <a href="https://zatoima.github.io/blog/llm/" class="tag-badge">LLM</a>
          
          <a href="https://zatoima.github.io/blog/ai/" class="tag-badge">AI</a>
          
          <a href="https://zatoima.github.io/blog/%E8%AB%96%E6%96%87/" class="tag-badge">è«–æ–‡</a>
          
        </div>
        
      </div>
      <div class="article-content">
        <h2 id="ã¯ã˜ã‚ã«">ã¯ã˜ã‚ã«</h2>
<p>æœ¬è¨˜äº‹ã¯2026-02-23æ™‚ç‚¹ã§ã®LLMé–¢é€£ã®æ³¨ç›®è«–æ–‡ã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚arXivã€Semantic Scholarã€Hugging Face Daily Papersã‹ã‚‰è‡ªå‹•åé›†ã—ã€Claude APIã§æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚</p>
<h2 id="1-viraasat-traversing-novel-paths-for-indian-cultural-reasoning">1. VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning</h2>
<ul>
<li><strong>è‘—è€…</strong>: Harshul Raj Surana, Arijit Maji, Aryan Vats, Akash Ghosh, Sriparna Saha ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-20</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="http://arxiv.org/abs/2602.18429v1">arxiv</a></li>
<li><strong>arXiv ID</strong>: 2602.18429v1</li>
</ul>
<h3 id="è¦ç´„">è¦ç´„</h3>
<p>VIRAASATã¯ã€ã‚¤ãƒ³ãƒ‰æ–‡åŒ–ã«é–¢ã™ã‚‹å¤šæ®µéšæ¨è«–ï¼ˆãƒãƒ«ãƒãƒ›ãƒƒãƒ—QAï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åŠè‡ªå‹•çš„ã«ç”Ÿæˆã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚ã‚‹ã€‚700ä»¥ä¸Šã®å°‚é–€å®¶ãŒç²¾é¸ã—ãŸæ–‡åŒ–çš„ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‹ã‚‰ãªã‚‹çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’æ´»ç”¨ã—ã€ã‚¤ãƒ³ãƒ‰ã®å…¨28å·ãƒ»8é€£é‚¦ç›´è½„é ˜ã«ã‚ãŸã‚‹13ã®æ–‡åŒ–å±æ€§ï¼ˆæ­´å²ã€ç¥­ã‚Šãªã©ï¼‰ã‚’ã‚«ãƒãƒ¼ã™ã‚‹3,200ä»¥ä¸Šã®ãƒãƒ«ãƒãƒ›ãƒƒãƒ—è³ªå•ã‚’ç”Ÿæˆã—ãŸã€‚æœ€å…ˆç«¯LLMã®è©•ä¾¡ã«ã‚ˆã‚Šã€Chain-of-Thoughtï¼ˆCoTï¼‰ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ä½é »åº¦ã®æ–‡åŒ–çš„äº‹å®Ÿã®çµ±åˆãƒ»æ ¹æ‹ ä»˜ã‘ã«é™ç•ŒãŒã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ãŸã€‚ã“ã®èª²é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€çŸ¥è­˜ã‚°ãƒ©ãƒ•ä¸Šã®åŸå­çš„æ“ä½œã‚’å†…éƒ¨çš„ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹Symbolic Chain-of-Manipulationï¼ˆSCoMï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦æ¨™æº–CoTãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æœ€å¤§20%ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã—ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.</p>
</details>
<h2 id="2-spq-an-ensemble-technique-for-large-language-model-compression">2. SPQ: An Ensemble Technique for Large Language Model Compression</h2>
<ul>
<li><strong>è‘—è€…</strong>: Jiamin Yao, Eren Gultepe</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-20</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="http://arxiv.org/abs/2602.18420v1">arxiv</a></li>
<li><strong>arXiv ID</strong>: 2602.18420v1</li>
</ul>
<h3 id="è¦ç´„-1">è¦ç´„</h3>
<p>æœ¬ç ”ç©¶ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®åœ§ç¸®æ‰‹æ³•ã¨ã—ã¦ã€åˆ†æ•£ä¿æŒå‹ç‰¹ç•°å€¤åˆ†è§£ï¼ˆSVDï¼‰ã€æ´»æ€§åŒ–ãƒ™ãƒ¼ã‚¹ã®æåˆˆã‚Šï¼ˆPruningï¼‰ã€å­¦ç¿’å¾Œç·šå½¢é‡å­åŒ–ï¼ˆQuantizationï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ€è¡“SPQã‚’ææ¡ˆã™ã‚‹ã€‚å„æ‰‹æ³•ã¯MLPã®å†—é•·ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é™¤å»ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å°„å½±ã®ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ã€å…¨ç·šå½¢å±¤ã®8ãƒ“ãƒƒãƒˆé‡å­åŒ–ã¨ã„ã†ãã‚Œãã‚Œç•°ãªã‚‹éåŠ¹ç‡æ€§ã‚’å¯¾è±¡ã¨ã—ã€åŒä¸€åœ§ç¸®ç‡ã«ãŠã„ã¦å˜ç‹¬æ‰‹æ³•ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚’é”æˆã™ã‚‹ã€‚LLaMA-2-7Bã¸ã®é©ç”¨ã§ã¯ã€æœ€å¤§75%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã‚’å®Ÿç¾ã—ã¤ã¤ã€WikiText-2ã®ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚’5.47ã‹ã‚‰4.91ã«æ”¹å–„ã—ã€C4ãƒ»TruthfulQAãƒ»GSM8Kãªã©ã®ä¸‹æµã‚¿ã‚¹ã‚¯ã§ã‚‚ç²¾åº¦ã‚’ç¶­æŒã—ãŸã€‚GPTQã‚„SparseGPTã¨ã„ã£ãŸå¼·åŠ›ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦ã‚‚ã€SPQã¯ã‚ˆã‚Šå°‘ãªã„ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆGPTQã®7.16GBã«å¯¾ã—6.86GBï¼‰ã§ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€æ¨è«–ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã§ã¯æœ€å¤§1.9å€ã®é«˜é€ŸåŒ–ã‚’é”æˆã—ã¦ãŠã‚Šã€ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ç’°å¢ƒã§ã®LLMå®Ÿç”¨å±•é–‹ã«æœ‰åŠ¹ãªæ‰‹æ³•ã§ã‚ã‚‹ã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ&rsquo;s robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: <a href="https://github.com/JiaminYao/SPQ_LLM_Compression/">https://github.com/JiaminYao/SPQ_LLM_Compression/</a></p>
</details>
<h2 id="3-subgroups-of-ud-induce-natural-rnn-and-transformer-architectures">3. Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures</h2>
<ul>
<li><strong>è‘—è€…</strong>: Joshua Nunley</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-20</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="http://arxiv.org/abs/2602.18417v1">arxiv</a></li>
<li><strong>arXiv ID</strong>: 2602.18417v1</li>
</ul>
<h3 id="è¦ç´„-2">è¦ç´„</h3>
<p>æœ¬è«–æ–‡ã¯ã€U(d)ã®é–‰éƒ¨åˆ†ç¾¤ä¸Šã«éš ã‚ŒçŠ¶æ…‹ã‚’æŒã¤ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã®ç›´æ¥çš„ãªæ çµ„ã¿ã‚’æç¤ºã™ã‚‹ã€‚æœ€å°é™ã®å…¬ç†çš„è¨­å®šã‹ã‚‰å‡ºç™ºã—ã€éƒ¨åˆ†ç¾¤ã®é¸æŠãŒçŠ¶æ…‹ç©ºé–“ãƒ»æ¥ç·šå°„å½±ãƒ»æ›´æ–°å†™åƒã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¤ãƒ³ç½®æ›ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹å…±é€šã®éª¨æ ¼ã‹ã‚‰ã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆå‹ãŠã‚ˆã³Transformerå‹ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å°å‡ºã™ã‚‹ã€‚O(d)ã«ç‰¹æ®ŠåŒ–ã—ãŸç›´äº¤çŠ¶æ…‹RNNãŠã‚ˆã³Transformerãƒ¢ãƒ‡ãƒ«ã‚’ã€Tiny Shakespeareã¨Penn Treebankã«ãŠã„ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’æƒãˆãŸæ¡ä»¶ã§è©•ä¾¡ã—ã¦ã„ã‚‹ã€‚ã•ã‚‰ã«ã€æ¥ç·šç©ºé–“ã«ãŠã‘ã‚‹ä¸€èˆ¬çš„ãªç·šå½¢æ··åˆæ‹¡å¼µã‚’å ±å‘Šã—ã¦ãŠã‚Šã€ã“ã‚Œã¯éƒ¨åˆ†ç¾¤ã®é¸æŠã«ä¾å­˜ã›ãšé©ç”¨å¯èƒ½ã§ã‚ã‚Šã€ç¾è¡Œã®O(d)å®Ÿé¨“ã«ãŠã„ã¦æœ‰é™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿äºˆç®—ä¸‹ã§ã®æ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.</p>
</details>
<h2 id="4-how-do-i--procedural-questions-predominate-student-llm-chatbot-conversations">4. &ldquo;How Do I &hellip;?&rdquo;: Procedural Questions Predominate Student-LLM Chatbot Conversations</h2>
<ul>
<li><strong>è‘—è€…</strong>: Alexandra Neagu, Marcus Messer, Peter Johnson, Rhodri Nelson</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-20</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="http://arxiv.org/abs/2602.18372v1">arxiv</a></li>
<li><strong>arXiv ID</strong>: 2602.18372v1</li>
</ul>
<h3 id="è¦ç´„-3">è¦ç´„</h3>
<p>æœ¬ç ”ç©¶ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ãƒ™ãƒ¼ã‚¹ã®æ•™è‚²ç”¨ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«å¯¾ã™ã‚‹å­¦ç”Ÿã®è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã€å½¢æˆçš„è‡ªä¸»å­¦ç¿’ã¨ç·æ‹¬çš„è©•ä¾¡èª²é¡Œã¨ã„ã†2ã¤ã®å­¦ç¿’æ–‡è„ˆã‹ã‚‰åˆ†æã—ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚6,113ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’11ç¨®ã®LLMã¨3åã®äººé–“è©•ä¾¡è€…ã«ã‚ˆã‚Šã€4ã¤ã®æ—¢å­˜åˆ†é¡ã‚¹ã‚­ãƒ¼ãƒã‚’ç”¨ã„ã¦åˆ†é¡ã—ãŸçµæœã€LLMã«ã‚ˆã‚‹è©•ä¾¡è€…é–“ä¿¡é ¼æ€§ã¯ä¸­ç¨‹åº¦ã‹ã‚‰è‰¯å¥½ã§ã‚ã‚Šã€äººé–“è©•ä¾¡è€…ã‚ˆã‚Šã‚‚ä¸€è²«æ€§ãŒé«˜ã„ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ä¸¡å­¦ç¿’æ–‡è„ˆã«ãŠã„ã¦ã€Œæ‰‹ç¶šãçš„ï¼ˆproceduralï¼‰ã€è³ªå•ãŒæœ€ã‚‚å¤šãã€ç‰¹ã«ç·æ‹¬çš„è©•ä¾¡ã®æº–å‚™æ™‚ã«ãã®å‚¾å‘ãŒé¡•è‘—ã§ã‚ã£ãŸã€‚ä¸€æ–¹ã§ã€æ—¢å­˜ã®åˆ†é¡ã‚¹ã‚­ãƒ¼ãƒã¯è¤‡åˆçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ„å‘³çš„è±Šã‹ã•ã«å¯¾å¿œã—ãã‚Œãšã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆçµ±åˆã®ãƒªã‚¹ã‚¯ã¨åˆ©ç‚¹ã®ç†è§£ã«ã¯é™ç•ŒãŒã‚ã‚‹ã“ã¨ã‚‚æ˜ã‚‰ã‹ã«ãªã£ãŸã€‚ä»Šå¾Œã¯ã€è«‡è©±å¿ƒç†å­¦ã«ãŠã‘ã‚‹ä¼šè©±åˆ†ææ‰‹æ³•ãªã©ã‚’é©ç”¨ã—ã€è¤‡æ•°ã‚¿ãƒ¼ãƒ³ã«ã‚ãŸã‚‹ä¼šè©±ã®å¾®å¦™ãªãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚’æ‰ãˆã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæ¨å¥¨ã•ã‚Œã¦ã„ã‚‹ã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Providing scaffolding through educational chatbots built on Large Language Models (LLM) has potential risks and benefits that remain an open area of research. When students navigate impasses, they ask for help by formulating impasse-driven questions. Within interactions with LLM chatbots, such questions shape the user prompts and drive the pedagogical effectiveness of the chatbot&rsquo;s response. This paper focuses on such student questions from two datasets of distinct learning contexts: formative self-study, and summative assessed coursework. We analysed 6,113 messages from both learning contexts, using 11 different LLMs and three human raters to classify student questions using four existing schemas. On the feasibility of using LLMs as raters, results showed moderate-to-good inter-rater reliability, with higher consistency than human raters. The data showed that &lsquo;procedural&rsquo; questions predominated in both learning contexts, but more so when students prepare for summative assessment. These results provide a basis on which to use LLMs for classification of student questions. However, we identify clear limitations in both the ability to classify with schemas and the value of doing so: schemas are limited and thus struggle to accommodate the semantic richness of composite prompts, offering only partial understanding the wider risks and benefits of chatbot integration. In the future, we recommend an analysis approach that captures the nuanced, multi-turn nature of conversation, for example, by applying methods from conversation analysis in discursive psychology.</p>
</details>
<h2 id="5-vichara-appellate-judgment-prediction-and-explanation-for-the-indian-judicial-system">5. Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System</h2>
<ul>
<li><strong>è‘—è€…</strong>: Pavithra PM Nair, Preethu Rose Anish</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-20</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="http://arxiv.org/abs/2602.18346v1">arxiv</a></li>
<li><strong>arXiv ID</strong>: 2602.18346v1</li>
</ul>
<h3 id="è¦ç´„-4">è¦ç´„</h3>
<p>Vicharaã¯ã€ã‚¤ãƒ³ãƒ‰ã®å¸æ³•åˆ¶åº¦ã«ãŠã‘ã‚‹æ§è¨´å¯©åˆ¤æ±ºã®äºˆæ¸¬ã¨èª¬æ˜ã‚’è¡Œã†æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹ã€‚æœ¬ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯è‹±èªã®æ§è¨´å¯©è¨´è¨Ÿæ–‡æ›¸ã‚’ã€Œæ±ºå®šãƒã‚¤ãƒ³ãƒˆã€ï¼ˆæ³•çš„äº‰ç‚¹ã€åˆ¤æ–­ä¸»ä½“ã€çµæœã€ç†ç”±ã€æ™‚é–“çš„æ–‡è„ˆã‚’å«ã‚€é›¢æ•£çš„ãªæ³•çš„åˆ¤æ–­ï¼‰ã«åˆ†è§£ã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸè¡¨ç¾ã«ã‚ˆã£ã¦æ­£ç¢ºãªäºˆæ¸¬ã¨è§£é‡ˆå¯èƒ½ãªèª¬æ˜ã‚’å®Ÿç¾ã™ã‚‹ã€‚èª¬æ˜ã®ç”Ÿæˆã«ã¯IRACï¼ˆIssue-Rule-Application-Conclusionï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ã‚¤ãƒ³ãƒ‰ã®æ³•çš„æ¨è«–ã«é©å¿œã•ã›ãŸæ§‹é€ åŒ–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚GPT-4o miniã€Llama-3.1-8Bã€Mistral-7Bã€Qwen2.5-7Bã®4ã¤ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦PredExãŠã‚ˆã³ILDC_expertãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©•ä¾¡ã—ãŸçµæœã€æ—¢å­˜ã®åˆ¤æ±ºäºˆæ¸¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ä¸Šå›ã‚Šã€GPT-4o miniãŒæœ€é«˜æ€§èƒ½ï¼ˆF1: PredExã§81.5ã€ILDC_expertã§80.3ï¼‰ã‚’é”æˆã—ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara&rsquo;s explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini&rsquo;s superior interpretability.</p>
</details>
<hr>
<p><em>ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚è«–æ–‡ã®è©³ç´°ã¯å„ã‚½ãƒ¼ã‚¹URLã‚’ã”å‚ç…§ãã ã•ã„ã€‚</em></p>

      </div>
    </div>

    
    
    <nav class="prev-next-nav">
      
      <a href="https://zatoima.github.io/snowflake-directory-table-integration/" class="prev-next-link prev-link">
        <span class="prev-next-label">â† å‰ã®è¨˜äº‹</span>
        <span class="prev-next-title">Snowflakeã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ†ãƒ¼ãƒ–ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æ›´æ–°</span>
      </a>
      
      
      <a href="https://zatoima.github.io/hugo-blog-mcp-server-llms-txt/" class="prev-next-link next-link">
        <span class="prev-next-label">æ¬¡ã®è¨˜äº‹ â†’</span>
        <span class="prev-next-title">Hugoè£½ãƒ–ãƒ­ã‚°ã«MCP Serverã¨llms.txtã‚’å®Ÿè£…ã™ã‚‹</span>
      </a>
      
    </nav>
    

    
<div class="share-buttons">
  <span class="share-label">å…±æœ‰:</span>
  <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2fzatoima.github.io%2fllm-papers-2026-02-23%2f&text=LLM%E8%AB%96%E6%96%87%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%EF%BC%882026-02-23%EF%BC%89" target="_blank" rel="noopener noreferrer" class="share-btn share-twitter" aria-label="Xã§å…±æœ‰">
    <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
  </a>
  <a href="https://b.hatena.ne.jp/entry/https://zatoima.github.io/llm-papers-2026-02-23/" target="_blank" rel="noopener noreferrer" class="share-btn share-hatena" aria-label="ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã«è¿½åŠ ">B!</a>
</div>






<div class="post-tags">
  
  <a href="https://zatoima.github.io/blog/llm/" class="tag-badge">LLM</a>
  
  <a href="https://zatoima.github.io/blog/ai/" class="tag-badge">AI</a>
  
  <a href="https://zatoima.github.io/blog/%E8%AB%96%E6%96%87/" class="tag-badge">è«–æ–‡</a>
  
</div>


  </article>

  
  <aside class="article-sidebar">
    <div class="toc-container">
      <div class="toc-title">ç›®æ¬¡</div>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#ã¯ã˜ã‚ã«">ã¯ã˜ã‚ã«</a></li>
    <li><a href="#1-viraasat-traversing-novel-paths-for-indian-cultural-reasoning">1. VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning</a>
      <ul>
        <li><a href="#è¦ç´„">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#2-spq-an-ensemble-technique-for-large-language-model-compression">2. SPQ: An Ensemble Technique for Large Language Model Compression</a>
      <ul>
        <li><a href="#è¦ç´„-1">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#3-subgroups-of-ud-induce-natural-rnn-and-transformer-architectures">3. Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures</a>
      <ul>
        <li><a href="#è¦ç´„-2">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#4-how-do-i--procedural-questions-predominate-student-llm-chatbot-conversations">4. &ldquo;How Do I &hellip;?&rdquo;: Procedural Questions Predominate Student-LLM Chatbot Conversations</a>
      <ul>
        <li><a href="#è¦ç´„-3">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#5-vichara-appellate-judgment-prediction-and-explanation-for-the-indian-judicial-system">5. Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System</a>
      <ul>
        <li><a href="#è¦ç´„-4">è¦ç´„</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
  </aside>
  
</div>

    </main>

    <footer class="site-footer">
      <div class="footer-inner">
        <div class="footer-description">memo blog</div>
        <div class="footer-links">
          <a href="/">ãƒ›ãƒ¼ãƒ </a>
          <a href="/blog/">è¨˜äº‹ä¸€è¦§</a>
          <a href="/tags/">ã‚¿ã‚°ä¸€è¦§</a>
          <a href="/about/">About</a>
        </div>
        <div class="footer-social">
          <a href="https://github.com/zatoima" target="_blank" rel="noopener noreferrer" aria-label="GitHub">
            <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
          </a>
          <a href="https://x.com/zatoima1" target="_blank" rel="noopener noreferrer" aria-label="X (Twitter)">
            <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
          </a>
          <a href="/index.xml" aria-label="RSS">
            <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M6.18 15.64a2.18 2.18 0 0 1 2.18 2.18C8.36 19.01 7.38 20 6.18 20C5 20 4 19.01 4 17.82a2.18 2.18 0 0 1 2.18-2.18M4 4.44A15.56 15.56 0 0 1 19.56 20h-2.83A12.73 12.73 0 0 0 4 7.27V4.44m0 5.66a9.9 9.9 0 0 1 9.9 9.9h-2.83A7.07 7.07 0 0 0 4 12.93V10.1z"/></svg>
          </a>
        </div>
        <div class="footer-copyright">Copyright Â© 2019, zatoima.</div>
      </div>
    </footer>
  </div>
  <script src="/js/lightbox.js" defer></script>
  <script src="/js/toc-highlight.js" defer></script>
  <script src="/js/code-copy.js" defer></script>
  <script src="/js/dark-mode.js" defer></script>
  <script src="/js/mobile-nav.js" defer></script>
  <script src="/js/search.js" defer></script>
</body>

</html>
