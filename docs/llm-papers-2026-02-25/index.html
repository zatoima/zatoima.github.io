<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://zatoima.github.io/favicon.ico" />
<title>LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-25ï¼‰ | my opinion is my own</title>
<meta name="title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-25ï¼‰" />
<meta name="description" content="" />
<meta name="keywords" content="LLM,AI,è«–æ–‡," />


<meta property="og:title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-25ï¼‰" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zatoima.github.io/llm-papers-2026-02-25/" /><meta property="og:image" content="https://zatoima.github.io/images/share.png"/><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2026-02-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2026-02-25T00:00:00+00:00" /><meta property="og:site_name" content="my opinion is my own" />




<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://zatoima.github.io/images/share.png"/>

<meta name="twitter:title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-25ï¼‰"/>
<meta name="twitter:description" content=""/>




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "\"LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-25ï¼‰\"",
  "description": "\"\"",
  "datePublished": "\"2026-02-25T00:00:00Z\"",
  "dateModified": "\"2026-02-25T00:00:00Z\"",
  "author": {
    "@type": "Person",
    "name": "\"zatoima\"",
    "url": "https://zatoima.github.io/about/"
  },
  "publisher": {
    "@type": "Person",
    "name": "\"zatoima\""
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "\"https://zatoima.github.io/llm-papers-2026-02-25/\""
  },
  "wordCount":  3396 ,
  "keywords": "[\"LLM\",\"AI\",\"è«–æ–‡\"]"
}
</script>

<meta name="referrer" content="no-referrer-when-downgrade" />

  <link rel="alternate" type="text/plain" href="/llms.txt" title="LLMs.txt" />
  <link rel="alternate" type="text/plain" href="/llms-full.txt" title="LLMs-full.txt" />

  <script>
    (function(){
      var t = localStorage.getItem('theme');
      if (t === 'dark' || ((!t || t === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
        document.documentElement.setAttribute('data-theme', 'dark');
      }
    })();
  </script><link rel="stylesheet" href="/css/zenn.css">

<script async src="https://www.googletagmanager.com/gtag/js?id=G-STFZ9QMXGM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-STFZ9QMXGM');
</script>
</head>

<body>
  <a href="#main" class="skip-link">ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¸ã‚¹ã‚­ãƒƒãƒ—</a>
  <div class="reading-progress"></div>
  <div class="site-wrapper">
    <header class="site-header">
      <div class="header-inner">
        <a href="/" class="site-logo">my opinion is my own</a>
        <nav class="header-nav">
<a href="/about/">About</a>
<a href="/blog/">Blog</a>
<a href="/index.xml">RSS</a>
<a href="/other/">Other</a>
<a href="/llms.txt" title="LLMs.txt - AI/LLMå‘ã‘ã‚µã‚¤ãƒˆæƒ…å ±">llms.txt</a>
</nav>
        <div class="header-actions">
          <button id="search-btn" class="header-icon-btn" aria-label="æ¤œç´¢">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><path d="m21 21-4.3-4.3"/></svg>
          </button>
          <button id="dark-mode-btn" class="header-icon-btn" aria-label="ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
          </button>
          <button id="hamburger-btn" class="hamburger-btn" aria-label="ãƒ¡ãƒ‹ãƒ¥ãƒ¼">
            <span></span><span></span><span></span>
          </button>
        </div>
      </div>
    </header>

    
    <div id="search-overlay" class="search-overlay">
      <div class="search-modal">
        <input type="text" id="search-input" class="search-input" placeholder="è¨˜äº‹ã‚’æ¤œç´¢... (Ctrl+K)" autocomplete="off">
        <div id="search-results" class="search-results"></div>
      </div>
    </div>

    
    <div id="nav-overlay" class="nav-overlay"></div>

    <main id="main" class="site-main">
<div class="article-layout">
  <article class="article-main">
    
    <nav class="breadcrumb" aria-label="ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆ">
  <a href="/">ãƒ›ãƒ¼ãƒ </a>
  <span class="breadcrumb-sep">&gt;</span>
  
  <a href="/blog/">Blog</a>
  <span class="breadcrumb-sep">&gt;</span>
  
  <span class="breadcrumb-current">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-25ï¼‰</span>
</nav>

    
    <div class="article-card">
      <div class="article-header">
        <div class="article-emoji"><img src="/images/tags/llm.svg" alt="LLM" class="tag-icon" loading="lazy"></div>
        <h1>LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-25ï¼‰</h1>
        
        <div class="article-meta">
          <time class="article-date" datetime='2026-02-25'>
            2026/02/25 ã«å…¬é–‹
          </time>
          
          
          
          <span class="reading-time">ğŸ“– ç´„6åˆ†</span>
        </div>
        <div class="article-tags">
          
          <a href="https://zatoima.github.io/blog/llm/" class="tag-badge"><img src="/images/tags/llm.svg" alt="LLM" class="tag-badge-icon" loading="lazy">LLM</a>
          
          <a href="https://zatoima.github.io/blog/ai/" class="tag-badge">AI</a>
          
          <a href="https://zatoima.github.io/blog/%E8%AB%96%E6%96%87/" class="tag-badge">è«–æ–‡</a>
          
        </div>
        
      </div>
      <div class="article-content">
        <h2 id="ã¯ã˜ã‚ã«">ã¯ã˜ã‚ã«</h2>
<p>æœ¬è¨˜äº‹ã¯2026-02-25æ™‚ç‚¹ã§ã®LLMé–¢é€£ã®æ³¨ç›®è«–æ–‡ã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚arXivã€Semantic Scholarã€Hugging Face Daily Papersã‹ã‚‰è‡ªå‹•åé›†ã—ã€Claude APIã§æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚</p>
<h2 id="1-a-very-big-video-reasoning-suite">1. A Very Big Video Reasoning Suite</h2>
<ul>
<li><strong>è‘—è€…</strong>: Maijunxian Wang, Ruisi Wang, Juyi Lin, Ran Ji, ThaddÃ¤us Wiedemer ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-23</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.20159">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.20159</li>
</ul>
<p><img src="paper_1.png" alt="A Very Big Video Reasoning Suite"></p>
<h3 id="è¦ç´„">è¦ç´„</h3>
<p>å‹•ç”»ãƒ¢ãƒ‡ãƒ«ã®æ€¥é€Ÿãªé€²æ­©ã¯ä¸»ã«è¦–è¦šçš„å“è³ªã«ç„¦ç‚¹ãŒå½“ã¦ã‚‰ã‚Œã¦ãŠã‚Šã€æ¨è«–èƒ½åŠ›ã®ç ”ç©¶ã¯ååˆ†ã«é€²ã‚“ã§ã„ãªã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€å‹•ç”»æ¨è«–ã®å¤§è¦æ¨¡å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ä¸è¶³ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€ä½“ç³»çš„ãªåˆ†é¡æ³•ã«åŸºã¥ã200ã®æ¨è«–ã‚¿ã‚¹ã‚¯ã¨100ä¸‡ä»¥ä¸Šã®å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã‚’å«ã‚€è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒVBVR Datasetã€ã‚’æ§‹ç¯‰ã—ãŸã€‚ã•ã‚‰ã«ã€ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡ã‚’è¶…ãˆã€ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‹ã¤äººé–“ã®åˆ¤æ–­ã¨æ•´åˆã—ãŸã‚¹ã‚³ã‚¢ãƒ©ãƒ¼ã‚’çµ„ã¿è¾¼ã‚“ã æ¤œè¨¼å¯èƒ½ãªè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ŒVBVR-Benchã€ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚ã“ã®VBVRã‚¹ã‚¤ãƒ¼ãƒˆã‚’æ´»ç”¨ã—ã¦å‹•ç”»æ¨è«–ã®å¤§è¦æ¨¡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç ”ç©¶ã‚’å®Ÿæ–½ã—ãŸçµæœã€æœªçŸ¥ã®æ¨è«–ã‚¿ã‚¹ã‚¯ã¸ã®å‰µç™ºçš„ãªæ±åŒ–ã®åˆæœŸå…†å€™ãŒè¦³å¯Ÿã•ã‚ŒãŸã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã€ãƒ¢ãƒ‡ãƒ«ã¯ã™ã¹ã¦å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at <a href="https://video-reason.com/">https://video-reason.com/</a> .</p>
</details>
<h2 id="2-vlanext-recipes-for-building-strong-vla-models">2. VLANeXt: Recipes for Building Strong VLA Models</h2>
<ul>
<li><strong>è‘—è€…</strong>: Xiao-Ming Wu, Bin Fan, Kang Liao, Jian-Jian Jiang, Runze Yang ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-20</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.18532">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.18532</li>
</ul>
<p><img src="paper_2.png" alt="VLANeXt: Recipes for Building Strong VLA Models"></p>
<h3 id="è¦ç´„-1">è¦ç´„</h3>
<p>å¤§è¦æ¨¡åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®å°é ­ã«ä¼´ã„ã€è¦–è¦šãƒ»è¨€èªç†è§£ã‚’æ´»ç”¨ã—ãŸæ±ç”¨çš„ãªæ–¹ç­–å­¦ç¿’ã®ãŸã‚ã®Vision-Language-Actionï¼ˆVLAï¼‰ãƒ¢ãƒ‡ãƒ«ãŒç™»å ´ã—ãŸãŒã€ç¾çŠ¶ã§ã¯å­¦ç¿’ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚„è©•ä¾¡è¨­å®šã®ä¸çµ±ä¸€ã«ã‚ˆã‚Šã€ã©ã®è¨­è¨ˆé¸æŠãŒé‡è¦ã‹ã®åˆ¤æ–­ãŒå›°é›£ã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€çµ±ä¸€çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨è©•ä¾¡ç’°å¢ƒã®ã‚‚ã¨ã§VLAã®è¨­è¨ˆç©ºé–“ã‚’å†æ¤œè¨ã—ã€RT-2ã‚„OpenVLAã«é¡ä¼¼ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‹ã‚‰å‡ºç™ºã—ã¦ã€åŸºç›¤ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ»çŸ¥è¦šã®è¦ç´ ãƒ»è¡Œå‹•ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®3ã¤ã®è¦³ç‚¹ã‹ã‚‰è¨­è¨ˆé¸æŠã‚’ä½“ç³»çš„ã«åˆ†æã—ãŸã€‚ã“ã®åˆ†æã‹ã‚‰ã€å¼·åŠ›ãªVLAãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã®ãŸã‚ã®å®Ÿè·µçš„ãªãƒ¬ã‚·ãƒ”ã¨ãªã‚‹12ã®é‡è¦ãªçŸ¥è¦‹ã‚’æŠ½å‡ºã—ãŸã€‚ãã®æˆæœã§ã‚ã‚‹VLANeXtã¯ã€LIBEROãŠã‚ˆã³LIBERO-plusãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å¾“æ¥ã®æœ€å…ˆç«¯æ‰‹æ³•ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã—ã€å®Ÿä¸–ç•Œå®Ÿé¨“ã§ã‚‚é«˜ã„æ±åŒ–èƒ½åŠ›ã‚’ç¤ºã—ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.</p>
</details>
<h2 id="3-topreward-token-probabilities-as-hidden-zero-shot-rewards-for-robotics">3. TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics</h2>
<ul>
<li><strong>è‘—è€…</strong>: Shirui Chen, Cole Harrison, Ying-Chun Lee, Angela Jin Yang, Zhongzheng Ren ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-22</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.19313">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.19313</li>
</ul>
<p><img src="paper_3.png" alt="TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics"></p>
<h3 id="è¦ç´„-2">è¦ç´„</h3>
<p>è¦–è¦šè¨€èªè¡Œå‹•ï¼ˆVLAï¼‰ãƒ¢ãƒ‡ãƒ«ã®å¼·åŒ–å­¦ç¿’ã«ãŠã‘ã‚‹ä½ã„ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã¨ã‚¹ãƒ‘ãƒ¼ã‚¹å ±é…¬ã®èª²é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ“ãƒ‡ã‚ªè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã®æ½œåœ¨çš„ãªä¸–ç•ŒçŸ¥è­˜ã‚’æ´»ç”¨ã—ã¦ãƒ­ãƒœãƒƒãƒˆã‚¿ã‚¹ã‚¯ã®é€²æ—ã‚’æ¨å®šã™ã‚‹æ–°ã—ã„æ™‚é–“çš„ä¾¡å€¤é–¢æ•°ã€ŒTOPRewardã€ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚å¾“æ¥ã®VLMã«é€²æ—å€¤ã‚’ç›´æ¥å‡ºåŠ›ã•ã›ã‚‹æ‰‹æ³•ãŒæ•°å€¤ã®èª¤è¡¨ç¾ã‚’èµ·ã“ã—ã‚„ã™ã„ã®ã«å¯¾ã—ã€TOPRewardã¯VLMå†…éƒ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‹ã‚‰ã‚¿ã‚¹ã‚¯é€²æ—ã‚’ç¢ºç‡çš„ã«æŠ½å‡ºã™ã‚‹ç‚¹ãŒç‰¹å¾´ã§ã‚ã‚‹ã€‚130ä»¥ä¸Šã®å®Ÿä¸–ç•Œã‚¿ã‚¹ã‚¯ã¨è¤‡æ•°ã®ãƒ­ãƒœãƒƒãƒˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆFrankaã€YAMã€SO-100/101ãªã©ï¼‰ã§ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè©•ä¾¡ã«ãŠã„ã¦ã€Qwen3-VLä¸Šã§å¹³å‡Value-Order Correlationï¼ˆVOCï¼‰0.947ã‚’é”æˆã—ã€åŒä¸€ãƒ¢ãƒ‡ãƒ«ã§ã»ã¼ã‚¼ãƒ­ã®ç›¸é–¢ã—ã‹å¾—ã‚‰ã‚Œãªã„æœ€å…ˆç«¯æ‰‹æ³•GVLã‚’å¤§å¹…ã«ä¸Šå›ã£ãŸã€‚ã•ã‚‰ã«ã€TOPRewardãŒæˆåŠŸæ¤œå‡ºã‚„å ±é…¬æ•´åˆå‹è¡Œå‹•ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã®ä¸‹æµå¿œç”¨ã«ã‚‚æœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã—ã¦ã„ã‚‹ã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM&rsquo;s internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.</p>
</details>
<h2 id="4-mancar-manifold-constrained-latent-reasoning-with-adaptive-test-time-computation-for-sequential-recommendation">4. ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation</h2>
<ul>
<li><strong>è‘—è€…</strong>: Kun Yang, Yuxuan Zhu, Yazhe Chen, Siyao Zheng, Bangyang Hong ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-23</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.20093">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.20093</li>
</ul>
<p><img src="paper_4.png" alt="ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation"></p>
<h3 id="è¦ç´„-3">è¦ç´„</h3>
<p>é€æ¬¡æ¨è–¦ã«ãŠã„ã¦ã€æ½œåœ¨ç©ºé–“ã§ã®å¤šæ®µéšæ¨è«–ã¯ãƒ†ã‚¹ãƒˆæ™‚è¨ˆç®—ã®å¼·åŒ–ã«æœ‰åŠ¹ã ãŒã€æ—¢å­˜æ‰‹æ³•ã¯ä¸­é–“æ¨è«–çŠ¶æ…‹ã«å¯¾ã™ã‚‹æ˜ç¤ºçš„ãªå®Ÿç¾å¯èƒ½æ€§åˆ¶ç´„ã‚’æ¬ ããŸã‚ã€æ¨è«–è»Œé“ãŒéç¾å®Ÿçš„ãªé ˜åŸŸã¸é€¸è„±ã™ã‚‹ã€Œæ½œåœ¨ãƒ‰ãƒªãƒ•ãƒˆã€å•é¡ŒãŒç”Ÿã˜ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€æ¨è–¦æ¨è«–ã‚’è‡ªç”±ãªæ½œåœ¨ç©ºé–“ã®ç²¾ç·»åŒ–ã§ã¯ãªãå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤šæ§˜ä½“ä¸Šã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã¨æ‰ãˆã€ManCARï¼ˆManifold-Constrained Adaptive Reasoningï¼‰ã‚’ææ¡ˆã™ã‚‹ã€‚ManCARã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚°ãƒ©ãƒ•ã®ãƒˆãƒãƒ­ã‚¸ãƒ¼ã«åŸºã¥ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›´è¿‘è¡Œå‹•ã®å”èª¿è¿‘å‚ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚·ãƒ³ãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ä¸Šã®åˆ†å¸ƒã¨ã—ã¦å±€æ‰€çš„ãªæ„å›³äº‹å‰åˆ†å¸ƒã‚’æ§‹ç¯‰ã—ã€è¨“ç·´æ™‚ã«æ½œåœ¨äºˆæ¸¬åˆ†å¸ƒã‚’ã“ã®äº‹å‰åˆ†å¸ƒã«æ®µéšçš„ã«æ•´åˆã•ã›ã‚‹ã“ã¨ã§æ¨è«–è»Œé“ã‚’æœ‰åŠ¹ãªå¤šæ§˜ä½“å†…ã«åˆ¶ç´„ã™ã‚‹ã€‚ãƒ†ã‚¹ãƒˆæ™‚ã«ã¯äºˆæ¸¬åˆ†å¸ƒãŒå®‰å®šã™ã‚‹ã¾ã§é©å¿œçš„ã«æ¨è«–ã‚’é€²ã‚ã€éå‰°ãªç²¾ç·»åŒ–ã‚’å›é¿ã™ã‚‹ã€‚7ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€ManCARã¯NDCG@10ã§æœ€å¤§46.88%ã®ç›¸å¯¾æ”¹å–„ã‚’é”æˆã—ã€æœ€å…ˆç«¯æ‰‹æ³•ã‚’ä¸€è²«ã—ã¦ä¸Šå›ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user&rsquo;s recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at <a href="https://github.com/FuCongResearchSquad/ManCAR">https://github.com/FuCongResearchSquad/ManCAR</a>.</p>
</details>
<h2 id="5-mobile-o-unified-multimodal-understanding-and-generation-on-mobile-device">5. Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device</h2>
<ul>
<li><strong>è‘—è€…</strong>: Abdelrahman Shaker, Ahmed Heakl, Jaseel Muhammad, Ritesh Thawkar, Omkar Thawakar ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-23</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.20161">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.20161</li>
</ul>
<p><img src="paper_5.png" alt="Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device"></p>
<h3 id="è¦ç´„-4">è¦ç´„</h3>
<p>çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã¯å˜ä¸€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§è¦–è¦šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç†è§£ã¨ç”Ÿæˆã®ä¸¡æ–¹ã‚’è¡Œãˆã‚‹ãŒã€æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ãƒ¼ã‚¿é‡ãŒå¤šãå¿…è¦ã§ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã¸ã®å±•é–‹ã«ã¯é‡ã™ãã‚‹ã€‚æœ¬è«–æ–‡ã§ã¯ã€çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çŸ¥èƒ½ã‚’ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹ä¸Šã§å®Ÿç¾ã™ã‚‹ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªãƒ“ã‚¸ãƒ§ãƒ³ãƒ»è¨€èªãƒ»æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã€ŒMobile-Oã€ã‚’ææ¡ˆã™ã‚‹ã€‚ä¸­æ ¸ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã‚ã‚‹Mobile Conditioning Projectorï¼ˆMCPï¼‰ã¯ã€æ·±ã•æ–¹å‘åˆ†é›¢å¯èƒ½ãªç•³ã¿è¾¼ã¿ã¨å±¤ã”ã¨ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’ç”¨ã„ã¦ã€ãƒ“ã‚¸ãƒ§ãƒ³ãƒ»è¨€èªç‰¹å¾´é‡ã¨æ‹¡æ•£ç”Ÿæˆå™¨ã‚’æœ€å°é™ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã§åŠ¹ç‡çš„ã«èåˆã™ã‚‹ã€‚æ•°ç™¾ä¸‡ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã§å­¦ç¿’ã—ã€æ–°ã—ã„4ã¤çµ„å½¢å¼ï¼ˆç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ç”»åƒã€è³ªå•ã€å›ç­”ï¼‰ã§å¾Œæ®µå­¦ç¿’ã‚’è¡Œã†ã“ã¨ã§ã€è¦–è¦šç†è§£ã¨ç”Ÿæˆèƒ½åŠ›ã‚’åŒæ™‚ã«å‘ä¸Šã•ã›ã‚‹ã€‚Mobile-Oã¯GenEvalã§74%ã‚’é”æˆã—Show-Oã‚„JanusFlowã‚’ãã‚Œãã‚Œ5%ãƒ»11%ä¸Šå›ã‚Šã¤ã¤6å€ãƒ»11å€é«˜é€Ÿã«å‹•ä½œã—ã€è¦–è¦šç†è§£ã§ã‚‚7ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¹³å‡ã§15.3%ãƒ»5.1%ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€iPhoneä¸Šã§512Ã—512ç”»åƒã‚’ç´„3ç§’ã§å‡¦ç†å¯èƒ½ãªã€ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ä¸Šã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç†è§£ãƒ»ç”Ÿæˆã®åˆã®å®Ÿç”¨çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç¢ºç«‹ã—ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at <a href="https://amshaker.github.io/Mobile-O/">https://amshaker.github.io/Mobile-O/</a></p>
</details>
<hr>
<p><em>ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚è«–æ–‡ã®è©³ç´°ã¯å„ã‚½ãƒ¼ã‚¹URLã‚’ã”å‚ç…§ãã ã•ã„ã€‚</em></p>

      </div>
    </div>

    
    
    
    <a href="https://github.com/zatoima/zatoima.github.io/edit/main/content/blog/2026-02-25-llm-papers-daily/index.md" target="_blank" rel="noopener noreferrer" class="github-edit-link">
      <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M11 4H4a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2v-7"/><path d="M18.5 2.5a2.121 2.121 0 0 1 3 3L12 15l-4 1 1-4 9.5-9.5z"/></svg>
      GitHubã§ç·¨é›†ã‚’ææ¡ˆ
    </a>
    
    

    
    
    <nav class="prev-next-nav">
      
      <a href="https://zatoima.github.io/mcp-server-context-usage-optimization/" class="prev-next-link prev-link">
        <span class="prev-next-label">â† å‰ã®è¨˜äº‹</span>
        <span class="prev-next-title">ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç³»MCPã‚µãƒ¼ãƒãƒ¼ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨é‡ã‚’æ¤œè¨¼ã—æ”¹å–„ã—ãŸ</span>
      </a>
      
      
      <div class="prev-next-link next-link empty"></div>
      
    </nav>
    

    
<div class="share-buttons">
  <span class="share-label">å…±æœ‰:</span>
  <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2fzatoima.github.io%2fllm-papers-2026-02-25%2f&text=LLM%E8%AB%96%E6%96%87%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%EF%BC%882026-02-25%EF%BC%89" target="_blank" rel="noopener noreferrer" class="share-btn share-twitter" aria-label="Xã§å…±æœ‰">
    <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
  </a>
  <a href="https://b.hatena.ne.jp/entry/https://zatoima.github.io/llm-papers-2026-02-25/" target="_blank" rel="noopener noreferrer" class="share-btn share-hatena" aria-label="ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã«è¿½åŠ ">B!</a>
</div>



<div class="related-articles">
  <h2 class="related-articles-title">é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„è¨˜äº‹</h2>
  <ul class="related-list">
    
    <li><a href="/llm-papers-2026-02-24/">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-24ï¼‰</a></li>
    
    <li><a href="/llm-papers-2026-02-23/">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-23ï¼‰</a></li>
    
    <li><a href="/mcp-server-context-usage-optimization/">ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç³»MCPã‚µãƒ¼ãƒãƒ¼ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨é‡ã‚’æ¤œè¨¼ã—æ”¹å–„ã—ãŸ</a></li>
    
    <li><a href="/llm-papers-pipeline-with-claude-code/">Claude Codeã§LLMè«–æ–‡ã®è‡ªå‹•åé›†ãƒ»è¦ç´„ãƒ»å…¬é–‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ãŸ</a></li>
    
  </ul>
</div>




<div class="post-tags">
  
  <a href="https://zatoima.github.io/blog/llm/" class="tag-badge"><img src="/images/tags/llm.svg" alt="LLM" class="tag-badge-icon" loading="lazy">LLM</a>
  
  <a href="https://zatoima.github.io/blog/ai/" class="tag-badge">AI</a>
  
  <a href="https://zatoima.github.io/blog/%E8%AB%96%E6%96%87/" class="tag-badge">è«–æ–‡</a>
  
</div>


  </article>

  
  <aside class="article-sidebar">
    <div class="toc-container">
      <div class="toc-title">ç›®æ¬¡</div>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#ã¯ã˜ã‚ã«">ã¯ã˜ã‚ã«</a></li>
    <li><a href="#1-a-very-big-video-reasoning-suite">1. A Very Big Video Reasoning Suite</a>
      <ul>
        <li><a href="#è¦ç´„">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#2-vlanext-recipes-for-building-strong-vla-models">2. VLANeXt: Recipes for Building Strong VLA Models</a>
      <ul>
        <li><a href="#è¦ç´„-1">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#3-topreward-token-probabilities-as-hidden-zero-shot-rewards-for-robotics">3. TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics</a>
      <ul>
        <li><a href="#è¦ç´„-2">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#4-mancar-manifold-constrained-latent-reasoning-with-adaptive-test-time-computation-for-sequential-recommendation">4. ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation</a>
      <ul>
        <li><a href="#è¦ç´„-3">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#5-mobile-o-unified-multimodal-understanding-and-generation-on-mobile-device">5. Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device</a>
      <ul>
        <li><a href="#è¦ç´„-4">è¦ç´„</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
  </aside>
  
</div>

    </main>

    <footer class="site-footer">
      <div class="footer-inner">
        <div class="footer-disclaimer">æœ¬ãƒ–ãƒ­ã‚°ã®å†…å®¹ã¯å€‹äººçš„ãªè¦‹è§£ã§ã‚ã‚Šã€æ‰€å±ã™ã‚‹ä¼æ¥­ãƒ»çµ„ç¹”ã®å…¬å¼ãªè¦‹è§£ã‚’ç¤ºã™ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</div>
        <div class="footer-links">
          <a href="/">ãƒ›ãƒ¼ãƒ </a>
          <a href="/blog/">è¨˜äº‹ä¸€è¦§</a>
          <a href="/tags/">ã‚¿ã‚°ä¸€è¦§</a>
          <a href="/about/">About</a>
        </div>
        <div class="footer-social">
          <a href="https://github.com/zatoima" target="_blank" rel="noopener noreferrer" aria-label="GitHub">
            <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
          </a>
          <a href="https://x.com/zatoima1" target="_blank" rel="noopener noreferrer" aria-label="X (Twitter)">
            <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
          </a>
          <a href="/index.xml" aria-label="RSS">
            <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M6.18 15.64a2.18 2.18 0 0 1 2.18 2.18C8.36 19.01 7.38 20 6.18 20C5 20 4 19.01 4 17.82a2.18 2.18 0 0 1 2.18-2.18M4 4.44A15.56 15.56 0 0 1 19.56 20h-2.83A12.73 12.73 0 0 0 4 7.27V4.44m0 5.66a9.9 9.9 0 0 1 9.9 9.9h-2.83A7.07 7.07 0 0 0 4 12.93V10.1z"/></svg>
          </a>
        </div>
        <div class="footer-copyright">Copyright Â© 2019, zatoima.</div>
        <div class="footer-disclaimer">æœ¬ãƒ–ãƒ­ã‚°ã®å†…å®¹ã¯å€‹äººçš„ãªè¦‹è§£ã§ã‚ã‚Šã€æ‰€å±ã™ã‚‹ä¼æ¥­ãƒ»çµ„ç¹”ã®å…¬å¼ãªè¦‹è§£ã‚’ç¤ºã™ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</div>
      </div>
    </footer>
  </div>
  <script src="/js/lightbox.js" defer></script>
  <script src="/js/toc-highlight.js" defer></script>
  <script src="/js/code-copy.js" defer></script>
  <script src="/js/dark-mode.js" defer></script>
  <script src="/js/mobile-nav.js" defer></script>
  <script src="/js/search.js" defer></script>
  <script src="/js/reading-progress.js" defer></script>
</body>

</html>
