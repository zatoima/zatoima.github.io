<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://zatoima.github.io/favicon.svg" />
<title>LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-27ï¼‰ | my opinion is my own</title>
<meta name="title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-27ï¼‰" />
<meta name="description" content="" />
<meta name="keywords" content="LLM,AI,è«–æ–‡," />


<meta property="og:url" content="https://zatoima.github.io/llm-papers-2026-02-27/">
  <meta property="og:site_name" content="my opinion is my own">
  <meta property="og:title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-27ï¼‰">
  <meta property="og:description" content="memo blog. Hugo on GitHub Pages">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2026-02-27T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-02-27T00:00:00+00:00">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="è«–æ–‡">
    <meta property="og:image" content="https://zatoima.github.io/images/share.png">




  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://zatoima.github.io/images/share.png">
  <meta name="twitter:title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-27ï¼‰">
  <meta name="twitter:description" content="memo blog. Hugo on GitHub Pages">




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "\"LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-27ï¼‰\"",
  "description": "\"\"",
  "datePublished": "\"2026-02-27T00:00:00Z\"",
  "dateModified": "\"2026-02-27T00:00:00Z\"",
  "author": {
    "@type": "Person",
    "name": "\"zatoima\"",
    "url": "https://zatoima.github.io/about/"
  },
  "publisher": {
    "@type": "Person",
    "name": "\"zatoima\""
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "\"https://zatoima.github.io/llm-papers-2026-02-27/\""
  },
  "wordCount":  3576 ,
  "keywords": "[\"LLM\",\"AI\",\"è«–æ–‡\"]"
}
</script>

<meta name="referrer" content="no-referrer-when-downgrade" />

  <link rel="alternate" type="text/plain" href="/llms.txt" title="LLMs.txt" />
  <link rel="alternate" type="text/plain" href="/llms-full.txt" title="LLMs-full.txt" />

  <script>
    (function(){
      var t = localStorage.getItem('theme');
      if (t === 'dark' || ((!t || t === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
        document.documentElement.setAttribute('data-theme', 'dark');
      }
    })();
  </script><link rel="stylesheet" href="/css/zenn.css">

<script async src="https://www.googletagmanager.com/gtag/js?id=G-STFZ9QMXGM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-STFZ9QMXGM');
</script>
</head>

<body>
  <a href="#main" class="skip-link">ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¸ã‚¹ã‚­ãƒƒãƒ—</a>
  <div class="reading-progress"></div>
  <div class="site-wrapper">
    <header class="site-header">
      <div class="header-inner">
        <a href="/" class="site-logo">my opinion is my own</a>
        <nav class="header-nav">
<a href="/about/">About</a>
<a href="/blog/">Blog</a>
<a href="/index.xml">RSS</a>
<a href="/other/">Other</a>
<a href="/llms.txt" title="LLMs.txt - AI/LLMå‘ã‘ã‚µã‚¤ãƒˆæƒ…å ±">llms.txt</a>
</nav>
        <div class="header-actions">
          <button id="search-btn" class="header-icon-btn" aria-label="æ¤œç´¢">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><path d="m21 21-4.3-4.3"/></svg>
          </button>
          <button id="dark-mode-btn" class="header-icon-btn" aria-label="ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
          </button>
          <button id="hamburger-btn" class="hamburger-btn" aria-label="ãƒ¡ãƒ‹ãƒ¥ãƒ¼">
            <span></span><span></span><span></span>
          </button>
        </div>
      </div>
    </header>

    
    <div id="search-overlay" class="search-overlay">
      <div class="search-modal">
        <input type="text" id="search-input" class="search-input" placeholder="è¨˜äº‹ã‚’æ¤œç´¢... (Ctrl+K)" autocomplete="off">
        <div id="search-results" class="search-results"></div>
      </div>
    </div>

    
    <div id="nav-overlay" class="nav-overlay"></div>

    <main id="main" class="site-main">
<div class="article-layout">
  <article class="article-main">
    
    <nav class="breadcrumb" aria-label="ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆ">
  <a href="/">ãƒ›ãƒ¼ãƒ </a>
  <span class="breadcrumb-sep">&gt;</span>
  
  <a href="/blog/">Blog</a>
  <span class="breadcrumb-sep">&gt;</span>
  
  <span class="breadcrumb-current">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-27ï¼‰</span>
</nav>

    
    <div class="article-card">
      <div class="article-header">
        <div class="article-emoji"><img src="/images/tags/llm.svg" alt="LLM" class="tag-icon" loading="lazy"></div>
        <h1>LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-27ï¼‰</h1>
        
        <div class="article-meta">
          <time class="article-date" datetime='2026-02-27'>
            2026/02/27 ã«å…¬é–‹
          </time>
          
          
          
          <span class="reading-time">ğŸ“– ç´„7åˆ†</span>
        </div>
        <div class="article-tags">
          
          <a href="https://zatoima.github.io/blog/llm/" class="tag-badge"><img src="/images/tags/llm.svg" alt="LLM" class="tag-badge-icon" loading="lazy">LLM</a>
          
          <a href="https://zatoima.github.io/blog/ai/" class="tag-badge"><img src="/images/tags/ai.svg" alt="AI" class="tag-badge-icon" loading="lazy">AI</a>
          
          <a href="https://zatoima.github.io/blog/%E8%AB%96%E6%96%87/" class="tag-badge">è«–æ–‡</a>
          
        </div>
        
      </div>
      <div class="article-content">
        <h2 id="ã¯ã˜ã‚ã«">ã¯ã˜ã‚ã«</h2>
<p>æœ¬è¨˜äº‹ã¯2026-02-27æ™‚ç‚¹ã§ã®LLMé–¢é€£ã®æ³¨ç›®è«–æ–‡ã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚arXivã€Semantic Scholarã€Hugging Face Daily Papersã‹ã‚‰è‡ªå‹•åé›†ã—ã€Claude APIã§æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚</p>
<h2 id="1-gui-libra-training-native-gui-agents-to-reason-and-act-with-action-aware-supervision-and-partially-verifiable-rl">1. GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL</h2>
<ul>
<li><strong>è‘—è€…</strong>: Rui Yang, Qianhui Wu, Zhaoyang Wang, Hanyang Chen, Ke Yang ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-25</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.22190">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.22190</li>
</ul>
<p><img src="paper_1.png" alt="GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL"></p>
<h3 id="è¦ç´„">è¦ç´„</h3>
<p>ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒã‚¤ãƒ†ã‚£ãƒ–GUIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€é•·æœŸçš„ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ã‚½ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã«ä¾ç„¶ã¨ã—ã¦é…ã‚Œã‚’å–ã£ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ãã®åŸå› ã¨ã—ã¦é«˜å“è³ªãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•´åˆå‹æ¨è«–ãƒ‡ãƒ¼ã‚¿ã®ä¸è¶³ã¨ã€GUIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå›ºæœ‰ã®èª²é¡Œã‚’ç„¡è¦–ã—ãŸæ±ç”¨çš„ãªå¾Œæ®µå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ç›´æ¥é©ç”¨ã¨ã„ã†2ã¤ã®é™ç•Œã‚’ç‰¹å®šã—ãŸã€‚ææ¡ˆæ‰‹æ³•GUI-Libraã§ã¯ã€81Kã®ç²¾é¸ã•ã‚ŒãŸGUIæ¨è«–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰ã€æ¨è«–ã¨ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¸¡ç«‹ã•ã›ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³èªè­˜å‹SFTï¼ˆæ¨è«–â†’ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ç›´æ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿æ··åˆãŠã‚ˆã³ãƒˆãƒ¼ã‚¯ãƒ³é‡ã¿ä»˜ã‘ï¼‰ã€ãã—ã¦éƒ¨åˆ†çš„æ¤œè¨¼å¯èƒ½æ€§ã®ä¸‹ã§RLã‚’å®‰å®šåŒ–ã•ã›ã‚‹ãŸã‚ã®KLæ­£å‰‡åŒ–ã¨æˆåŠŸé©å¿œå‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å°å…¥ã—ãŸã€‚Webãƒ»ãƒ¢ãƒã‚¤ãƒ«ã®å¤šæ§˜ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ãŠã„ã¦ã€ã‚¹ãƒ†ãƒƒãƒ—å˜ä½ã®ç²¾åº¦ã¨ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ã‚¿ã‚¹ã‚¯å®Œäº†ç‡ã®ä¸¡æ–¹ã§ä¸€è²«ã—ãŸæ”¹å–„ã‚’é”æˆã—ã€ã‚³ã‚¹ãƒˆã®ã‹ã‹ã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ãªã—ã«å¤§å¹…ãªæ€§èƒ½å‘ä¸ŠãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.</p>
</details>
<h2 id="2-quantvla-scale-calibrated-post-training-quantization-for-vision-language-action-models">2. QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models</h2>
<ul>
<li><strong>è‘—è€…</strong>: Jingxuan Zhang, Yunta Hsieh, Zhongwei Wang, Haokun Lin, Xin Wang ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-23</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.20309">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.20309</li>
</ul>
<p><img src="paper_2.png" alt="QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models"></p>
<h3 id="è¦ç´„-1">è¦ç´„</h3>
<p>QuantVLAã¯ã€è¦–è¦šãƒ»è¨€èªãƒ»è¡Œå‹•ï¼ˆVLAï¼‰ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹åˆã®å­¦ç¿’ä¸è¦ãªãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é‡å­åŒ–ï¼ˆPTQï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€æ‹¡æ•£ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆDiTï¼‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®é‡å­åŒ–ã«ã‚‚åˆã‚ã¦æˆåŠŸã—ãŸæ‰‹æ³•ã§ã‚ã‚‹ã€‚æœ¬ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€é¸æŠçš„é‡å­åŒ–ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã€æ³¨æ„æ¸©åº¦ãƒãƒƒãƒãƒ³ã‚°ã€å‡ºåŠ›ãƒ˜ãƒƒãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°ã¨ã„ã†3ã¤ã®ã‚¹ã‚±ãƒ¼ãƒ«æ ¡æ­£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ„ã¿è¾¼ã‚“ã§ãŠã‚Šã€è¿½åŠ å­¦ç¿’ãªã—ã«å°‘é‡ã®ãƒ©ãƒ™ãƒ«ãªã—ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§é©ç”¨å¯èƒ½ã§ã‚ã‚‹ã€‚LIBEROãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ä¸Šã®ä»£è¡¨çš„ãªVLAãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€QuantVLAã¯ãƒ•ãƒ«ç²¾åº¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ã‚’ä¸Šå›ã‚Šã¤ã¤ã€é‡å­åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ç´„70%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã¨1.22å€ã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ¨è«–é«˜é€ŸåŒ–ã‚’é”æˆã—ã€è¨ˆç®—ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»é›»åŠ›åˆ¶ç´„ä¸‹ã§ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªä½ãƒ“ãƒƒãƒˆèº«ä½“çŸ¥èƒ½ã¸ã®å®Ÿç”¨çš„ãªé“ç­‹ã‚’ç¤ºã—ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.</p>
</details>
<h2 id="3-vecglypher-unified-vector-glyph-generation-with-language-models">3. VecGlypher: Unified Vector Glyph Generation with Language Models</h2>
<ul>
<li><strong>è‘—è€…</strong>: Xiaoke Huang, Bhavul Gauri, Kam Woh Ng, Tony Ng, Mengmeng Xu ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-25</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.21461">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.21461</li>
</ul>
<p><img src="paper_3.png" alt="VecGlypher: Unified Vector Glyph Generation with Language Models"></p>
<h3 id="è¦ç´„-2">è¦ç´„</h3>
<p>ãƒ™ã‚¯ã‚¿ãƒ¼ã‚°ãƒªãƒ•ï¼ˆãƒ‡ã‚¸ã‚¿ãƒ«ã‚¿ã‚¤ãƒã‚°ãƒ©ãƒ•ã‚£ã®åŸºæœ¬å˜ä½ï¼‰ã®ç”Ÿæˆã«ãŠã„ã¦ã€å¾“æ¥ã®å­¦ç¿’ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ã¯æ‰‹ä½œæ¥­ã§ç”¨æ„ã—ãŸè¦‹æœ¬ã‚·ãƒ¼ãƒˆã‚„ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ»ãƒ™ã‚¯ã‚¿ãƒ¼å¤‰æ›ã®å¾Œå‡¦ç†ã«ä¾å­˜ã—ã¦ãŠã‚Šã€ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚„ç·¨é›†æ€§ã«åˆ¶ç´„ãŒã‚ã£ãŸã€‚æœ¬è«–æ–‡ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ã‚„ç”»åƒè¦‹æœ¬ã‹ã‚‰é«˜å“è³ªãªãƒ™ã‚¯ã‚¿ãƒ¼ã‚°ãƒªãƒ•ã‚’SVGãƒ‘ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ç›´æ¥è‡ªå·±å›å¸°çš„ã«ç”Ÿæˆã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒVecGlypherã€ã‚’ææ¡ˆã™ã‚‹ã€‚å­¦ç¿’ã«ã¯ã€39Kã®ãƒã‚¤ã‚ºã‚’å«ã‚€Envatoãƒ•ã‚©ãƒ³ãƒˆã«ã‚ˆã‚‹å¤§è¦æ¨¡ç¶™ç¶šå­¦ç¿’ã§SVGæ§‹æ–‡ã¨é•·è·é›¢ã®å¹¾ä½•æ§‹é€ ã‚’ç¿’å¾—ã—ãŸå¾Œã€2.5Kã®å°‚é–€å®¶ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãGoogle Fontsã§è¨€èªãƒ»ç”»åƒã¨å¹¾ä½•æƒ…å ±ã®æ•´åˆã‚’å›³ã‚‹2æ®µéšãƒ¬ã‚·ãƒ”ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚ã‚¯ãƒ­ã‚¹ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®åˆ†å¸ƒå¤–è©•ä¾¡ã«ãŠã„ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®ç”Ÿæˆã§ã¯æ±ç”¨LLMã‚„å°‚é–€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’å¤§å¹…ã«ä¸Šå›ã‚Šã€ç”»åƒå‚ç…§ç”Ÿæˆã§ã¯DeepVecFont-v2ã‚„DualVectorã‚’è¶…ãˆã‚‹æœ€å…ˆç«¯æ€§èƒ½ã‚’é”æˆã—ãŸã€‚æœ¬æ‰‹æ³•ã«ã‚ˆã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯è¨€è‘‰ã‚„è¦‹æœ¬ç”»åƒã ã‘ã§ãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­è¨ˆã§ãã€å°†æ¥ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«ã¸ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªåŸºç›¤ã‚’æä¾›ã™ã‚‹ã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Vector glyphs are the atomic units of digital typography, yet most learning-based pipelines still depend on carefully curated exemplar sheets and raster-to-vector postprocessing, which limits accessibility and editability. We introduce VecGlypher, a single multimodal language model that generates high-fidelity vector glyphs directly from text descriptions or image exemplars. Given a style prompt, optional reference glyph images, and a target character, VecGlypher autoregressively emits SVG path tokens, avoiding raster intermediates and producing editable, watertight outlines in one pass. A typography-aware data and training recipe makes this possible: (i) a large-scale continuation stage on 39K noisy Envato fonts to master SVG syntax and long-horizon geometry, followed by (ii) post-training on 2.5K expert-annotated Google Fonts with descriptive tags and exemplars to align language and imagery with geometry; preprocessing normalizes coordinate frames, canonicalizes paths, de-duplicates families, and quantizes coordinates for stable long-sequence decoding. On cross-family OOD evaluation, VecGlypher substantially outperforms both general-purpose LLMs and specialized vector-font baselines for text-only generation, while image-referenced generation reaches a state-of-the-art performance, with marked gains over DeepVecFont-v2 and DualVector. Ablations show that model scale and the two-stage recipe are critical and that absolute-coordinate serialization yields the best geometry. VecGlypher lowers the barrier to font creation by letting users design with words or exemplars, and provides a scalable foundation for future multimodal design tools.</p>
</details>
<h2 id="4-javisdit-unified-modeling-and-optimization-for-joint-audio-video-generation">4. JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation</h2>
<ul>
<li><strong>è‘—è€…</strong>: Kai Liu, Yanhao Zheng, Kai Wang, Shengqiong Wu, Rongjunchen Zhang ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-22</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.19163">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.19163</li>
</ul>
<p><img src="paper_4.png" alt="JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation"></p>
<h3 id="è¦ç´„-3">è¦ç´„</h3>
<p>AIGCãŒãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒç”Ÿæˆã‚’è¶…ãˆã¦å‹•ç”»ãƒ»éŸ³å£°ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åˆæˆã¸ã¨æ‹¡å¤§ã™ã‚‹ä¸­ã€ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ã‹ã‚‰åŒæœŸãƒ»æ„å‘³æ•´åˆã•ã‚ŒãŸéŸ³å£°ã¨æ˜ åƒã‚’åŒæ™‚ç”Ÿæˆã™ã‚‹ã€Œå…±åŒéŸ³å£°å‹•ç”»ç”Ÿæˆï¼ˆJAVGï¼‰ã€ãŒé‡è¦ãªèª²é¡Œã¨ãªã£ã¦ã„ã‚‹ã€‚æœ¬è«–æ–‡ã§ã¯ã€JavisDiT++ã¨ã„ã†JAVGã®çµ±ä¸€çš„ãªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¨æœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£å›ºæœ‰ã®Mixture-of-Expertsï¼ˆMS-MoEï¼‰è¨­è¨ˆã«ã‚ˆã‚Šã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ãªç›¸äº’ä½œç”¨ã‚’ä¿ƒé€²ã—ã¤ã¤å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ç”Ÿæˆå“è³ªã‚’å‘ä¸Šã•ã›ã¦ã„ã‚‹ã€‚ã•ã‚‰ã«ã€éŸ³å£°ãƒ»å‹•ç”»ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«ã§ã®æ˜ç¤ºçš„ãªæ™‚é–“åŒæœŸã‚’å®Ÿç¾ã™ã‚‹Temporal-Aligned RoPEï¼ˆTA-RoPEï¼‰æˆ¦ç•¥ã¨ã€å“è³ªãƒ»ä¸€è²«æ€§ãƒ»åŒæœŸæ€§ã®å„æ¬¡å…ƒã§äººé–“ã®é¸å¥½ã«ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’æ•´åˆã•ã›ã‚‹Audio-Video Direct Preference Optimizationï¼ˆAV-DPOï¼‰æ‰‹æ³•ã‚’å°å…¥ã—ã¦ã„ã‚‹ã€‚Wan2.1-1.3B-T2Vã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã€ç´„100ä¸‡ä»¶ã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’ã—ãŸæœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€å®šæ€§çš„ãƒ»å®šé‡çš„è©•ä¾¡ã®ä¸¡é¢ã§æ—¢å­˜æ‰‹æ³•ã‚’å¤§å¹…ã«ä¸Šå›ã‚Šã€æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã—ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>AIGC has rapidly expanded from text-to-image generation toward high-quality multimodal synthesis across video and audio. Within this context, joint audio-video generation (JAVG) has emerged as a fundamental task that produces synchronized and semantically aligned sound and vision from textual descriptions. However, compared with advanced commercial models such as Veo3, existing open-source methods still suffer from limitations in generation quality, temporal synchrony, and alignment with human preferences. To bridge the gap, this paper presents JavisDiT++, a concise yet powerful framework for unified modeling and optimization of JAVG. First, we introduce a modality-specific mixture-of-experts (MS-MoE) design that enables cross-modal interaction efficacy while enhancing single-modal generation quality. Then, we propose a temporal-aligned RoPE (TA-RoPE) strategy to achieve explicit, frame-level synchronization between audio and video tokens. Besides, we develop an audio-video direct preference optimization (AV-DPO) method to align model outputs with human preference across quality, consistency, and synchrony dimensions. Built upon Wan2.1-1.3B-T2V, our model achieves state-of-the-art performance merely with around 1M public training entries, significantly outperforming prior approaches in both qualitative and quantitative evaluations. Comprehensive ablation studies have been conducted to validate the effectiveness of our proposed modules. All the code, model, and dataset are released at <a href="https://JavisVerse.github.io/JavisDiT2-page">https://JavisVerse.github.io/JavisDiT2-page</a>.</p>
</details>
<h2 id="5-dualpath-breaking-the-storage-bandwidth-bottleneck-in-agentic-llm-inference">5. DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference</h2>
<ul>
<li><strong>è‘—è€…</strong>: Yongtong Wu, Shaoyuan Chen, Yinmin Zhong, Rilin Huang, Yixuan Tan ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-25</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.21548">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.21548</li>
</ul>
<p><img src="paper_5.png" alt="DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference"></p>
<h3 id="è¦ç´„-4">è¦ç´„</h3>
<p>ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‹LLMæ¨è«–ã§ã¯ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸I/OãŒæ€§èƒ½ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨ãªã£ã¦ãŠã‚Šã€åˆ†é›¢å‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ãŠã„ã¦ãƒ—ãƒªãƒ•ã‚£ãƒ«ã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸NICãŒå¸¯åŸŸé£½å’Œã™ã‚‹ä¸€æ–¹ã€ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³å´ã¯éŠä¼‘çŠ¶æ…‹ã«ãªã‚‹ã¨ã„ã†éå¯¾ç§°æ€§ãŒç”Ÿã˜ã¦ã„ã‚‹ã€‚æœ¬è«–æ–‡ã§ã¯ã€ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹DualPathã‚’ææ¡ˆã—ã€å¾“æ¥ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ãƒ—ãƒªãƒ•ã‚£ãƒ«ã¸ã®çµŒè·¯ã«åŠ ãˆã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³ã‚’çµŒç”±ã—RDMAã§ãƒ—ãƒªãƒ•ã‚£ãƒ«ã‚¨ãƒ³ã‚¸ãƒ³ã¸è»¢é€ã™ã‚‹æ–°ãŸãªçµŒè·¯ã‚’å°å…¥ã™ã‚‹ã€‚ã“ã®äºŒé‡çµŒè·¯æ–¹å¼ã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¼»è¼³ã‚„ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã«æ•æ„Ÿãªãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œé€šä¿¡ã¸ã®å¹²æ¸‰ã‚’å›é¿ã§ãã€ã•ã‚‰ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ã‚ˆã‚Šãƒ—ãƒªãƒ•ã‚£ãƒ«ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³é–“ã®è² è·ã‚’å‹•çš„ã«åˆ†æ•£ã™ã‚‹ã€‚æœ¬ç•ªã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‹ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚’ç”¨ã„ãŸ3ãƒ¢ãƒ‡ãƒ«ã§ã®è©•ä¾¡ã§ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¨è«–ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒæœ€å¤§1.87å€ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒSLOã‚’ç¶­æŒã—ã¤ã¤å¹³å‡1.96å€ã«å‘ä¸Šã—ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput.
We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path &ndash; which inherently avoids network congestion and avoids interference with latency-critical model execution communications &ndash; with a global scheduler that dynamically balances load across prefill and decode engines.
Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87times on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96times without violating SLO.</p>
</details>
<hr>
<p><em>ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚è«–æ–‡ã®è©³ç´°ã¯å„ã‚½ãƒ¼ã‚¹URLã‚’ã”å‚ç…§ãã ã•ã„ã€‚</em></p>

      </div>
    </div>

    
    
    
    <a href="https://github.com/zatoima/zatoima.github.io/edit/main/content/blog/2026-02-27-llm-papers-daily/index.md" target="_blank" rel="noopener noreferrer" class="github-edit-link">
      <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M11 4H4a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2v-7"/><path d="M18.5 2.5a2.121 2.121 0 0 1 3 3L12 15l-4 1 1-4 9.5-9.5z"/></svg>
      GitHubã§ç·¨é›†ã‚’ææ¡ˆ
    </a>
    
    

    
    
    <nav class="prev-next-nav">
      
      <a href="https://zatoima.github.io/llm-papers-2026-02-26/" class="prev-next-link prev-link">
        <span class="prev-next-label">â† å‰ã®è¨˜äº‹</span>
        <span class="prev-next-title">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-26ï¼‰</span>
      </a>
      
      
      <div class="prev-next-link next-link empty"></div>
      
    </nav>
    

    
<div class="share-buttons">
  <span class="share-label">å…±æœ‰:</span>
  <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2fzatoima.github.io%2fllm-papers-2026-02-27%2f&text=LLM%E8%AB%96%E6%96%87%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%EF%BC%882026-02-27%EF%BC%89" target="_blank" rel="noopener noreferrer" class="share-btn share-twitter" aria-label="Xã§å…±æœ‰">
    <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
  </a>
  <a href="https://b.hatena.ne.jp/entry/https://zatoima.github.io/llm-papers-2026-02-27/" target="_blank" rel="noopener noreferrer" class="share-btn share-hatena" aria-label="ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã«è¿½åŠ ">B!</a>
</div>



<div class="related-articles">
  <h2 class="related-articles-title">é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„è¨˜äº‹</h2>
  <ul class="related-list">
    
    <li><a href="/llm-papers-2026-02-26/">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-26ï¼‰</a></li>
    
    <li><a href="/llm-papers-2026-02-25/">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-25ï¼‰</a></li>
    
    <li><a href="/llm-papers-2026-02-24/">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-24ï¼‰</a></li>
    
    <li><a href="/llm-papers-2026-02-23/">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-23ï¼‰</a></li>
    
    <li><a href="/claude-code-agent-teams-parallel-collaboration/">Claude Codeã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ æ©Ÿèƒ½ - è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹ä¸¦åˆ—å”èª¿ä½œæ¥­</a></li>
    
  </ul>
</div>




<div class="post-tags">
  
  <a href="https://zatoima.github.io/blog/llm/" class="tag-badge"><img src="/images/tags/llm.svg" alt="LLM" class="tag-badge-icon" loading="lazy">LLM</a>
  
  <a href="https://zatoima.github.io/blog/ai/" class="tag-badge"><img src="/images/tags/ai.svg" alt="AI" class="tag-badge-icon" loading="lazy">AI</a>
  
  <a href="https://zatoima.github.io/blog/%E8%AB%96%E6%96%87/" class="tag-badge">è«–æ–‡</a>
  
</div>


  </article>

  
  <aside class="article-sidebar">
    <div class="toc-container">
      <div class="toc-title">ç›®æ¬¡</div>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#ã¯ã˜ã‚ã«">ã¯ã˜ã‚ã«</a></li>
    <li><a href="#1-gui-libra-training-native-gui-agents-to-reason-and-act-with-action-aware-supervision-and-partially-verifiable-rl">1. GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL</a>
      <ul>
        <li><a href="#è¦ç´„">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#2-quantvla-scale-calibrated-post-training-quantization-for-vision-language-action-models">2. QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models</a>
      <ul>
        <li><a href="#è¦ç´„-1">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#3-vecglypher-unified-vector-glyph-generation-with-language-models">3. VecGlypher: Unified Vector Glyph Generation with Language Models</a>
      <ul>
        <li><a href="#è¦ç´„-2">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#4-javisdit-unified-modeling-and-optimization-for-joint-audio-video-generation">4. JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation</a>
      <ul>
        <li><a href="#è¦ç´„-3">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#5-dualpath-breaking-the-storage-bandwidth-bottleneck-in-agentic-llm-inference">5. DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference</a>
      <ul>
        <li><a href="#è¦ç´„-4">è¦ç´„</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
  </aside>
  
</div>

    </main>

    <footer class="site-footer">
      <div class="footer-inner">
        <div class="footer-disclaimer">æœ¬ãƒ–ãƒ­ã‚°ã®å†…å®¹ã¯å€‹äººçš„ãªè¦‹è§£ã§ã‚ã‚Šã€æ‰€å±ã™ã‚‹ä¼æ¥­ãƒ»çµ„ç¹”ã®å…¬å¼ãªè¦‹è§£ã‚’ç¤ºã™ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</div>
        <div class="footer-links">
          <a href="/">ãƒ›ãƒ¼ãƒ </a>
          <a href="/blog/">è¨˜äº‹ä¸€è¦§</a>
          <a href="/tags/">ã‚¿ã‚°ä¸€è¦§</a>
          <a href="/about/">About</a>
        </div>
        <div class="footer-social">
          <a href="https://github.com/zatoima" target="_blank" rel="noopener noreferrer" aria-label="GitHub">
            <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
          </a>
          <a href="https://x.com/zatoima1" target="_blank" rel="noopener noreferrer" aria-label="X (Twitter)">
            <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
          </a>
          <a href="/index.xml" aria-label="RSS">
            <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M6.18 15.64a2.18 2.18 0 0 1 2.18 2.18C8.36 19.01 7.38 20 6.18 20C5 20 4 19.01 4 17.82a2.18 2.18 0 0 1 2.18-2.18M4 4.44A15.56 15.56 0 0 1 19.56 20h-2.83A12.73 12.73 0 0 0 4 7.27V4.44m0 5.66a9.9 9.9 0 0 1 9.9 9.9h-2.83A7.07 7.07 0 0 0 4 12.93V10.1z"/></svg>
          </a>
        </div>
        <div class="footer-copyright">Copyright Â© 2019, zatoima.</div>
        <div class="footer-disclaimer">memo blog. Hugo on GitHub Pages</div>
        <div class="footer-disclaimer">æœ¬ãƒ–ãƒ­ã‚°ã®å†…å®¹ã¯å€‹äººçš„ãªè¦‹è§£ã§ã‚ã‚Šã€æ‰€å±ã™ã‚‹ä¼æ¥­ãƒ»çµ„ç¹”ã®å…¬å¼ãªè¦‹è§£ã‚’ç¤ºã™ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</div>
      </div>
    </footer>
  </div>
  <script src="/js/lightbox.js" defer></script>
  <script src="/js/toc-highlight.js" defer></script>
  <script src="/js/code-copy.js" defer></script>
  <script src="/js/dark-mode.js" defer></script>
  <script src="/js/mobile-nav.js" defer></script>
  <script src="/js/search.js" defer></script>
  <script src="/js/reading-progress.js" defer></script>
</body>

</html>
