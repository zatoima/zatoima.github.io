<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://zatoima.github.io/favicon.svg" />
<title>LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-26ï¼‰ | my opinion is my own</title>
<meta name="title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-26ï¼‰" />
<meta name="description" content="" />
<meta name="keywords" content="LLM,AI,è«–æ–‡," />


<meta property="og:url" content="https://zatoima.github.io/llm-papers-2026-02-26/">
  <meta property="og:site_name" content="my opinion is my own">
  <meta property="og:title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-26ï¼‰">
  <meta property="og:description" content="memo blog. Hugo on GitHub Pages">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2026-02-26T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-02-26T00:00:00+00:00">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="è«–æ–‡">
    <meta property="og:image" content="https://zatoima.github.io/images/share.png">




  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://zatoima.github.io/images/share.png">
  <meta name="twitter:title" content="LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-26ï¼‰">
  <meta name="twitter:description" content="memo blog. Hugo on GitHub Pages">




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "\"LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-26ï¼‰\"",
  "description": "\"\"",
  "datePublished": "\"2026-02-26T00:00:00Z\"",
  "dateModified": "\"2026-02-26T00:00:00Z\"",
  "author": {
    "@type": "Person",
    "name": "\"zatoima\"",
    "url": "https://zatoima.github.io/about/"
  },
  "publisher": {
    "@type": "Person",
    "name": "\"zatoima\""
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "\"https://zatoima.github.io/llm-papers-2026-02-26/\""
  },
  "wordCount":  3285 ,
  "keywords": "[\"LLM\",\"AI\",\"è«–æ–‡\"]"
}
</script>

<meta name="referrer" content="no-referrer-when-downgrade" />

  <link rel="alternate" type="text/plain" href="/llms.txt" title="LLMs.txt" />
  <link rel="alternate" type="text/plain" href="/llms-full.txt" title="LLMs-full.txt" />

  <script>
    (function(){
      var t = localStorage.getItem('theme');
      if (t === 'dark' || ((!t || t === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
        document.documentElement.setAttribute('data-theme', 'dark');
      }
    })();
  </script><link rel="stylesheet" href="/css/zenn.css">

<script async src="https://www.googletagmanager.com/gtag/js?id=G-STFZ9QMXGM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-STFZ9QMXGM');
</script>
</head>

<body>
  <a href="#main" class="skip-link">ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¸ã‚¹ã‚­ãƒƒãƒ—</a>
  <div class="reading-progress"></div>
  <div class="site-wrapper">
    <header class="site-header">
      <div class="header-inner">
        <a href="/" class="site-logo">my opinion is my own</a>
        <nav class="header-nav">
<a href="/about/">About</a>
<a href="/blog/">Blog</a>
<a href="/index.xml">RSS</a>
<a href="/other/">Other</a>
<a href="/llms.txt" title="LLMs.txt - AI/LLMå‘ã‘ã‚µã‚¤ãƒˆæƒ…å ±">llms.txt</a>
</nav>
        <div class="header-actions">
          <button id="search-btn" class="header-icon-btn" aria-label="æ¤œç´¢">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><path d="m21 21-4.3-4.3"/></svg>
          </button>
          <button id="dark-mode-btn" class="header-icon-btn" aria-label="ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
          </button>
          <button id="hamburger-btn" class="hamburger-btn" aria-label="ãƒ¡ãƒ‹ãƒ¥ãƒ¼">
            <span></span><span></span><span></span>
          </button>
        </div>
      </div>
    </header>

    
    <div id="search-overlay" class="search-overlay">
      <div class="search-modal">
        <input type="text" id="search-input" class="search-input" placeholder="è¨˜äº‹ã‚’æ¤œç´¢... (Ctrl+K)" autocomplete="off">
        <div id="search-results" class="search-results"></div>
      </div>
    </div>

    
    <div id="nav-overlay" class="nav-overlay"></div>

    <main id="main" class="site-main">
<div class="article-layout">
  <article class="article-main">
    
    <nav class="breadcrumb" aria-label="ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆ">
  <a href="/">ãƒ›ãƒ¼ãƒ </a>
  <span class="breadcrumb-sep">&gt;</span>
  
  <a href="/blog/">Blog</a>
  <span class="breadcrumb-sep">&gt;</span>
  
  <span class="breadcrumb-current">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-26ï¼‰</span>
</nav>

    
    <div class="article-card">
      <div class="article-header">
        <div class="article-emoji"><img src="/images/tags/llm.svg" alt="LLM" class="tag-icon" loading="lazy"></div>
        <h1>LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-26ï¼‰</h1>
        
        <div class="article-meta">
          <time class="article-date" datetime='2026-02-26'>
            2026/02/26 ã«å…¬é–‹
          </time>
          
          
          
          <span class="reading-time">ğŸ“– ç´„6åˆ†</span>
        </div>
        <div class="article-tags">
          
          <a href="https://zatoima.github.io/blog/llm/" class="tag-badge"><img src="/images/tags/llm.svg" alt="LLM" class="tag-badge-icon" loading="lazy">LLM</a>
          
          <a href="https://zatoima.github.io/blog/ai/" class="tag-badge"><img src="/images/tags/ai.svg" alt="AI" class="tag-badge-icon" loading="lazy">AI</a>
          
          <a href="https://zatoima.github.io/blog/%E8%AB%96%E6%96%87/" class="tag-badge">è«–æ–‡</a>
          
        </div>
        
      </div>
      <div class="article-content">
        <h2 id="ã¯ã˜ã‚ã«">ã¯ã˜ã‚ã«</h2>
<p>æœ¬è¨˜äº‹ã¯2026-02-26æ™‚ç‚¹ã§ã®LLMé–¢é€£ã®æ³¨ç›®è«–æ–‡ã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚arXivã€Semantic Scholarã€Hugging Face Daily Papersã‹ã‚‰è‡ªå‹•åé›†ã—ã€Claude APIã§æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚</p>
<h2 id="1-query-focused-and-memory-aware-reranker-for-long-context-processing">1. Query-focused and Memory-aware Reranker for Long Context Processing</h2>
<ul>
<li><strong>è‘—è€…</strong>: Yuqing Li, Jiangnan Li, Mo Yu, Guoxuan Ding, Zheng Lin ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-12</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.12192">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.12192</li>
</ul>
<p><img src="paper_1.png" alt="Query-focused and Memory-aware Reranker for Long Context Processing"></p>
<h3 id="è¦ç´„">è¦ç´„</h3>
<p>å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹æ¤œç´¢ãƒ˜ãƒƒãƒ‰ï¼ˆretrieval headsï¼‰ã®åˆ†æã«åŸºã¥ãã€é¸æŠã•ã‚ŒãŸãƒ˜ãƒƒãƒ‰ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’ç”¨ã„ã¦ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚¯ã‚¨ãƒªã®é–¢é€£æ€§ã‚’æ¨å®šã™ã‚‹ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚ã“ã®æ‰‹æ³•ã¯ãƒªã‚¹ãƒˆãƒ¯ã‚¤ã‚ºæ–¹å¼ã«ã‚ˆã‚Šå€™è£œãƒªã‚¹ãƒˆå…¨ä½“ã®æƒ…å ±ã‚’æ´»ç”¨ã—ã€é€£ç¶šçš„ãªé–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’è‡ªç„¶ã«ç”Ÿæˆã™ã‚‹ãŸã‚ã€Likertã‚¹ã‚±ãƒ¼ãƒ«ã®æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã‚’å¿…è¦ã¨ã›ãšä»»æ„ã®æ¤œç´¢ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’å¯èƒ½ã§ã‚ã‚‹ã€‚4Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¨‹åº¦ã®å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã‚‚é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹è»½é‡ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€Wikipediaã‚„é•·æ–‡ãƒŠãƒ©ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å«ã‚€è¤‡æ•°ãƒ‰ãƒ¡ã‚¤ãƒ³ã§æ—¢å­˜ã®ãƒã‚¤ãƒ³ãƒˆãƒ¯ã‚¤ã‚ºãƒ»ãƒªã‚¹ãƒˆãƒ¯ã‚¤ã‚ºãƒªãƒ©ãƒ³ã‚«ãƒ¼ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã—ãŸã€‚ã•ã‚‰ã«å¯¾è©±ç†è§£ã¨è¨˜æ†¶æ´»ç”¨èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹LoCoMoãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ–°ãŸãªæœ€é«˜æ€§èƒ½ã‚’è¨˜éŒ²ã—ã€å€™è£œãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã¸ã®æ–‡è„ˆæƒ…å ±ã®ä»˜åŠ ã‚„ä¸­é–“å±¤ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®æ´»ç”¨ã¨ã„ã£ãŸæŸ”è»Ÿãªæ‹¡å¼µã«ã‚‚å¯¾å¿œã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.</p>
</details>
<h2 id="2-pyvision-rl-forging-open-agentic-vision-models-via-rl">2. PyVision-RL: Forging Open Agentic Vision Models via RL</h2>
<ul>
<li><strong>è‘—è€…</strong>: Shitian Zhao, Shaoheng Lin, Ming Li, Haoquan Zhang, Wenshuo Peng ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-24</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.20739">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.20739</li>
</ul>
<p><img src="paper_2.png" alt="PyVision-RL: Forging Open Agentic Vision Models via RL"></p>
<h3 id="è¦ç´„-1">è¦ç´„</h3>
<p>å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚„ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³æ¨è«–ã‚’æ¸›ã‚‰ã™ã€Œã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å´©å£Šã€ãŒèª²é¡Œã¨ãªã£ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«å‘ã‘å¼·åŒ–å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ŒPyVision-RLã€ã‚’ææ¡ˆã—ã€ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆæˆ¦ç•¥ã¨ç´¯ç©ãƒ„ãƒ¼ãƒ«å ±é…¬ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€å´©å£Šã‚’é˜²ããƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’ä¿ƒé€²ã™ã‚‹ã€‚çµ±ä¸€çš„ãªå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚Šã€ç”»åƒç†è§£ç”¨ã®PyVision-Imageã¨å‹•ç”»ç†è§£ç”¨ã®PyVision-Videoã‚’é–‹ç™ºã—ãŸã€‚ç‰¹ã«PyVision-Videoã§ã¯ã€æ¨è«–ä¸­ã«ã‚¿ã‚¹ã‚¯ã«é–¢é€£ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é¸æŠçš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã«ã‚ˆã‚Šã€è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã®ä½¿ç”¨é‡ã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹ã€‚å®Ÿé¨“ã®çµæœã€é«˜ã„æ€§èƒ½ã¨åŠ¹ç‡ã®å‘ä¸ŠãŒç¢ºèªã•ã‚Œã€æŒç¶šçš„ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã®è¦–è¦šå‡¦ç†ãŒã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ä¸å¯æ¬ ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.</p>
</details>
<h2 id="3-from-perception-to-action-an-interactive-benchmark-for-vision-reasoning">3. From Perception to Action: An Interactive Benchmark for Vision Reasoning</h2>
<ul>
<li><strong>è‘—è€…</strong>: Yuhao Wu, Maojia Song, Yihuai Lan, Lei Wang, Zhiqiang Hu ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-24</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.21015">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.21015</li>
</ul>
<p><img src="paper_3.png" alt="From Perception to Action: An Interactive Benchmark for Vision Reasoning"></p>
<h3 id="è¦ç´„-2">è¦ç´„</h3>
<p>ç¾å®Ÿä¸–ç•Œã®å¿œç”¨ï¼ˆembodiedã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³ã€é•·æœŸçš„æ“ä½œãªã©ï¼‰ã«ã¯ç‰©ç†çš„æ§‹é€ ã®ç†è§£ãŒä¸å¯æ¬ ã§ã‚ã‚‹ãŒã€æ—¢å­˜ã®Vision-Language Modelï¼ˆVLMï¼‰è©•ä¾¡ã¯æ§‹é€ ã‚’è€ƒæ…®ã—ãªã„å˜ä¸€ã‚¿ãƒ¼ãƒ³ã®è¨­å®šï¼ˆVQAãªã©ï¼‰ã«åã£ã¦ãŠã‚Šã€å‹•çš„ç’°å¢ƒã«ãŠã‘ã‚‹å¹¾ä½•å­¦ãƒ»æ¥è§¦ãƒ»æ”¯æŒé–¢ä¿‚ã«åŸºã¥ãè¡Œå‹•æ¨è«–èƒ½åŠ›ã‚’é©åˆ‡ã«è©•ä¾¡ã§ãã¦ã„ãªã„ã€‚ã“ã®èª²é¡Œã«å¯¾ã—ã€è‘—è€…ã‚‰ã¯CHAINï¼ˆCausal Hierarchy of Actions and Interactionsï¼‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ææ¡ˆã—ãŸã€‚ã“ã‚Œã¯ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é§†å‹•ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãª3Dãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒç‰©ç†çš„åˆ¶ç´„ã«åŸºã¥ã„ã¦æ§‹é€ åŒ–ã•ã‚ŒãŸè¡Œå‹•ç³»åˆ—ã‚’ç†è§£ãƒ»è¨ˆç”»ãƒ»å®Ÿè¡Œã§ãã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹ã€‚CHAINã¯å—å‹•çš„ãªçŸ¥è¦šã‹ã‚‰èƒ½å‹•çš„ãªå•é¡Œè§£æ±ºã¸ã¨è©•ä¾¡ã‚’è»¢æ›ã—ã€é€£å‹•ã™ã‚‹æ©Ÿæ¢°ãƒ‘ã‚ºãƒ«ã‚„3Dç©ã¿ä¸Šã’ãƒ»ãƒ‘ãƒƒã‚­ãƒ³ã‚°ãªã©ã®ã‚¿ã‚¹ã‚¯ã‚’å«ã‚€ã€‚æœ€å…ˆç«¯ã®VLMãŠã‚ˆã³æ‹¡æ•£ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’çµ±ä¸€çš„ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è¨­å®šã§è©•ä¾¡ã—ãŸçµæœã€ãƒˆãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã§ã•ãˆç‰©ç†æ§‹é€ ã‚„å› æœçš„åˆ¶ç´„ã®å†…åœ¨åŒ–ã«è‹¦æˆ¦ã—ã€ä¿¡é ¼æ€§ã®ã‚ã‚‹é•·æœŸè¨ˆç”»ã®ç”Ÿæˆã‚„ã€çŸ¥è¦šã—ãŸæ§‹é€ ã‚’åŠ¹æœçš„ãªè¡Œå‹•ã«å¤‰æ›ã™ã‚‹ã“ã¨ãŒå›°é›£ã§ã‚ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents&rsquo; ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at <a href="https://social-ai-studio.github.io/CHAIN/">https://social-ai-studio.github.io/CHAIN/</a>.</p>
</details>
<h2 id="4-multi-vector-index-compression-in-any-modality">4. Multi-Vector Index Compression in Any Modality</h2>
<ul>
<li><strong>è‘—è€…</strong>: Hanxiang Qin, Alexander Martin, Rohan Jha, Chunsheng Zuo, Reno Kriz ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-24</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.21202">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.21202</li>
</ul>
<p><img src="paper_4.png" alt="Multi-Vector Index Compression in Any Modality"></p>
<h3 id="è¦ç´„-3">è¦ç´„</h3>
<p>æœ¬ç ”ç©¶ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒãƒ»å‹•ç”»ãªã©ä»»æ„ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ãŠã‘ã‚‹é…å»¶ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å‹æ¤œç´¢ã®ãŸã‚ã®ã€åŠ¹ç‡çš„ãªãƒãƒ«ãƒãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åœ§ç¸®æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚é…å»¶ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã¯æƒ…å ±æ¤œç´¢ã®ä¸»è¦ãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¨ãªã£ã¦ã„ã‚‹ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆãŒæ–‡æ›¸é•·ã«æ¯”ä¾‹ã—ã¦å¢—å¤§ã™ã‚‹ã¨ã„ã†èª²é¡ŒãŒã‚ã‚‹ã€‚ã“ã®å•é¡Œã«å¯¾ã—ã€ã‚¯ã‚¨ãƒªéä¾å­˜ã®ãƒãƒ«ãƒãƒ™ã‚¯ãƒˆãƒ«æ–‡æ›¸è¡¨ç¾åœ§ç¸®æ‰‹æ³•ã¨ã—ã¦ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒªã‚µã‚¤ã‚ºã€ãƒ¡ãƒ¢ãƒªãƒˆãƒ¼ã‚¯ãƒ³ã€éšå±¤çš„ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã€ãŠã‚ˆã³æ–°æ‰‹æ³•ã§ã‚ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³èª˜å°ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆAGCï¼‰ã®4ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚AGCã¯ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã‚’ç”¨ã„ã¦æ–‡æ›¸ä¸­ã®æ„å‘³çš„ã«é‡è¦ãªé ˜åŸŸã‚’ã‚¯ãƒ©ã‚¹ã‚¿é‡å¿ƒã¨ã—ã¦ç‰¹å®šã—ã€ãƒˆãƒ¼ã‚¯ãƒ³é›†ç´„ã®é‡ã¿ä»˜ã‘ã‚’è¡Œã†æ‰‹æ³•ã§ã‚ã‚‹ã€‚ãƒ†ã‚­ã‚¹ãƒˆï¼ˆBEIRï¼‰ã€è¦–è¦šæ–‡æ›¸ï¼ˆViDoReï¼‰ã€å‹•ç”»ï¼ˆMSR-VTTã€MultiVENT 2.0ï¼‰ã®æ¤œç´¢ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡ã—ãŸçµæœã€AGCã¯ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯åœ§ç¸®æ‰‹æ³•ã‚’ä¸€è²«ã—ã¦ä¸Šå›ã‚Šã€éåœ§ç¸®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’é”æˆã—ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.</p>
</details>
<h2 id="5-learning-cross-view-object-correspondence-via-cycle-consistent-mask-prediction">5. Learning Cross-View Object Correspondence via Cycle-Consistent Mask Prediction</h2>
<ul>
<li><strong>è‘—è€…</strong>: Shannan Yan, Leqi Zheng, Keyu Lv, Jingchen Ni, Hongyang Wei ã»ã‹</li>
<li><strong>å…¬é–‹æ—¥</strong>: 2026-02-22</li>
<li><strong>ã‚½ãƒ¼ã‚¹</strong>: <a href="https://arxiv.org/abs/2602.18996">huggingface</a></li>
<li><strong>arXiv ID</strong>: 2602.18996</li>
</ul>
<p><img src="paper_5.png" alt="Learning Cross-View Object Correspondence via Cycle-Consistent Mask Prediction"></p>
<h3 id="è¦ç´„-4">è¦ç´„</h3>
<p>æœ¬ç ”ç©¶ã§ã¯ã€å‹•ç”»ä¸­ã®ç•°ãªã‚‹è¦–ç‚¹é–“ï¼ˆç‰¹ã«ä¸€äººç§°è¦–ç‚¹ã¨ä¸‰äººç§°è¦–ç‚¹ã®ç›¸äº’å¤‰æ›ï¼‰ã«ãŠã‘ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ¬ãƒ™ãƒ«ã®è¦–è¦šçš„å¯¾å¿œä»˜ã‘ã‚¿ã‚¹ã‚¯ã«å–ã‚Šçµ„ã‚“ã§ã„ã‚‹ã€‚ææ¡ˆæ‰‹æ³•ã¯æ¡ä»¶ä»˜ãäºŒå€¤ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«åŸºã¥ãã‚·ãƒ³ãƒ—ãƒ«ã‹ã¤åŠ¹æœçš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€ã‚¯ã‚¨ãƒªã¨ãªã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒã‚¹ã‚¯ã‚’æ½œåœ¨è¡¨ç¾ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå‹•ç”»ä¸­ã®å¯¾å¿œã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½ç½®ç‰¹å®šã‚’è¡Œã†ã€‚è¦–ç‚¹ã«ä¸å¤‰ãªé ‘å¥ãªè¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã€ã‚µã‚¤ã‚¯ãƒ«æ•´åˆæ€§ã«åŸºã¥ãå­¦ç¿’ç›®çš„é–¢æ•°ã‚’å°å…¥ã—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¦–ç‚¹ã§äºˆæ¸¬ã•ã‚ŒãŸãƒã‚¹ã‚¯ã‚’ã‚½ãƒ¼ã‚¹è¦–ç‚¹ã«é€†æŠ•å½±ã—ã¦å…ƒã®ã‚¯ã‚¨ãƒªãƒã‚¹ã‚¯ã‚’å†æ§‹æˆã™ã‚‹åŒæ–¹å‘åˆ¶ç´„ã‚’èª²ã—ã¦ã„ã‚‹ã€‚ã“ã®åˆ¶ç´„ã¯æ­£è§£ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¿…è¦ã¨ã—ãªã„å¼·åŠ›ãªè‡ªå·±æ•™å¸«ä¿¡å·ã¨ã—ã¦æ©Ÿèƒ½ã—ã€æ¨è«–æ™‚ã®ãƒ†ã‚¹ãƒˆæ™‚å­¦ç¿’ï¼ˆTTTï¼‰ã‚‚å¯èƒ½ã«ã™ã‚‹ã€‚Ego-Exo4DãŠã‚ˆã³HANDAL-Xãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€ææ¡ˆæ‰‹æ³•ã®æœ€é©åŒ–ç›®çš„é–¢æ•°ã¨TTTæˆ¦ç•¥ã®æœ‰åŠ¹æ€§ãŒç¤ºã•ã‚Œã€æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã—ãŸã€‚</p>
<details>
  <summary>åŸæ–‡Abstract</summary>
  <p>We study the task of establishing object-level visual correspondence across different viewpoints in videos, focusing on the challenging egocentric-to-exocentric and exocentric-to-egocentric scenarios. We propose a simple yet effective framework based on conditional binary segmentation, where an object query mask is encoded into a latent representation to guide the localization of the corresponding object in a target video. To encourage robust, view-invariant representations, we introduce a cycle-consistency training objective: the predicted mask in the target view is projected back to the source view to reconstruct the original query mask. This bidirectional constraint provides a strong self-supervisory signal without requiring ground-truth annotations and enables test-time training (TTT) at inference. Experiments on the Ego-Exo4D and HANDAL-X benchmarks demonstrate the effectiveness of our optimization objective and TTT strategy, achieving state-of-the-art performance. The code is available at <a href="https://github.com/shannany0606/CCMP">https://github.com/shannany0606/CCMP</a>.</p>
</details>
<hr>
<p><em>ã“ã®è¨˜äº‹ã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚è«–æ–‡ã®è©³ç´°ã¯å„ã‚½ãƒ¼ã‚¹URLã‚’ã”å‚ç…§ãã ã•ã„ã€‚</em></p>

      </div>
    </div>

    
    
    
    <a href="https://github.com/zatoima/zatoima.github.io/edit/main/content/blog/2026-02-26-llm-papers-daily/index.md" target="_blank" rel="noopener noreferrer" class="github-edit-link">
      <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M11 4H4a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2v-7"/><path d="M18.5 2.5a2.121 2.121 0 0 1 3 3L12 15l-4 1 1-4 9.5-9.5z"/></svg>
      GitHubã§ç·¨é›†ã‚’ææ¡ˆ
    </a>
    
    

    
    
    <nav class="prev-next-nav">
      
      <a href="https://zatoima.github.io/snowflake-mcp-documentation-server-benchmark/" class="prev-next-link prev-link">
        <span class="prev-next-label">â† å‰ã®è¨˜äº‹</span>
        <span class="prev-next-title">Snowflake MCPãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚µãƒ¼ãƒãƒ¼2ç¨®ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ</span>
      </a>
      
      
      <div class="prev-next-link next-link empty"></div>
      
    </nav>
    

    
<div class="share-buttons">
  <span class="share-label">å…±æœ‰:</span>
  <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2fzatoima.github.io%2fllm-papers-2026-02-26%2f&text=LLM%E8%AB%96%E6%96%87%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%EF%BC%882026-02-26%EF%BC%89" target="_blank" rel="noopener noreferrer" class="share-btn share-twitter" aria-label="Xã§å…±æœ‰">
    <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
  </a>
  <a href="https://b.hatena.ne.jp/entry/https://zatoima.github.io/llm-papers-2026-02-26/" target="_blank" rel="noopener noreferrer" class="share-btn share-hatena" aria-label="ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã«è¿½åŠ ">B!</a>
</div>



<div class="related-articles">
  <h2 class="related-articles-title">é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„è¨˜äº‹</h2>
  <ul class="related-list">
    
    <li><a href="/llm-papers-2026-02-25/">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-25ï¼‰</a></li>
    
    <li><a href="/llm-papers-2026-02-24/">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-24ï¼‰</a></li>
    
    <li><a href="/llm-papers-2026-02-23/">LLMè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆ2026-02-23ï¼‰</a></li>
    
    <li><a href="/claude-code-agent-teams-parallel-collaboration/">Claude Codeã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ æ©Ÿèƒ½ - è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹ä¸¦åˆ—å”èª¿ä½œæ¥­</a></li>
    
    <li><a href="/mcp-server-context-usage-optimization/">ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç³»MCPã‚µãƒ¼ãƒãƒ¼ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨é‡ã‚’æ¤œè¨¼ã—æ”¹å–„ã—ãŸ</a></li>
    
  </ul>
</div>




<div class="post-tags">
  
  <a href="https://zatoima.github.io/blog/llm/" class="tag-badge"><img src="/images/tags/llm.svg" alt="LLM" class="tag-badge-icon" loading="lazy">LLM</a>
  
  <a href="https://zatoima.github.io/blog/ai/" class="tag-badge"><img src="/images/tags/ai.svg" alt="AI" class="tag-badge-icon" loading="lazy">AI</a>
  
  <a href="https://zatoima.github.io/blog/%E8%AB%96%E6%96%87/" class="tag-badge">è«–æ–‡</a>
  
</div>


  </article>

  
  <aside class="article-sidebar">
    <div class="toc-container">
      <div class="toc-title">ç›®æ¬¡</div>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#ã¯ã˜ã‚ã«">ã¯ã˜ã‚ã«</a></li>
    <li><a href="#1-query-focused-and-memory-aware-reranker-for-long-context-processing">1. Query-focused and Memory-aware Reranker for Long Context Processing</a>
      <ul>
        <li><a href="#è¦ç´„">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#2-pyvision-rl-forging-open-agentic-vision-models-via-rl">2. PyVision-RL: Forging Open Agentic Vision Models via RL</a>
      <ul>
        <li><a href="#è¦ç´„-1">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#3-from-perception-to-action-an-interactive-benchmark-for-vision-reasoning">3. From Perception to Action: An Interactive Benchmark for Vision Reasoning</a>
      <ul>
        <li><a href="#è¦ç´„-2">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#4-multi-vector-index-compression-in-any-modality">4. Multi-Vector Index Compression in Any Modality</a>
      <ul>
        <li><a href="#è¦ç´„-3">è¦ç´„</a></li>
      </ul>
    </li>
    <li><a href="#5-learning-cross-view-object-correspondence-via-cycle-consistent-mask-prediction">5. Learning Cross-View Object Correspondence via Cycle-Consistent Mask Prediction</a>
      <ul>
        <li><a href="#è¦ç´„-4">è¦ç´„</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
  </aside>
  
</div>

    </main>

    <footer class="site-footer">
      <div class="footer-inner">
        <div class="footer-disclaimer">æœ¬ãƒ–ãƒ­ã‚°ã®å†…å®¹ã¯å€‹äººçš„ãªè¦‹è§£ã§ã‚ã‚Šã€æ‰€å±ã™ã‚‹ä¼æ¥­ãƒ»çµ„ç¹”ã®å…¬å¼ãªè¦‹è§£ã‚’ç¤ºã™ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</div>
        <div class="footer-links">
          <a href="/">ãƒ›ãƒ¼ãƒ </a>
          <a href="/blog/">è¨˜äº‹ä¸€è¦§</a>
          <a href="/tags/">ã‚¿ã‚°ä¸€è¦§</a>
          <a href="/about/">About</a>
        </div>
        <div class="footer-social">
          <a href="https://github.com/zatoima" target="_blank" rel="noopener noreferrer" aria-label="GitHub">
            <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
          </a>
          <a href="https://x.com/zatoima1" target="_blank" rel="noopener noreferrer" aria-label="X (Twitter)">
            <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
          </a>
          <a href="/index.xml" aria-label="RSS">
            <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M6.18 15.64a2.18 2.18 0 0 1 2.18 2.18C8.36 19.01 7.38 20 6.18 20C5 20 4 19.01 4 17.82a2.18 2.18 0 0 1 2.18-2.18M4 4.44A15.56 15.56 0 0 1 19.56 20h-2.83A12.73 12.73 0 0 0 4 7.27V4.44m0 5.66a9.9 9.9 0 0 1 9.9 9.9h-2.83A7.07 7.07 0 0 0 4 12.93V10.1z"/></svg>
          </a>
        </div>
        <div class="footer-copyright">Copyright Â© 2019, zatoima.</div>
        <div class="footer-disclaimer">memo blog. Hugo on GitHub Pages</div>
        <div class="footer-disclaimer">æœ¬ãƒ–ãƒ­ã‚°ã®å†…å®¹ã¯å€‹äººçš„ãªè¦‹è§£ã§ã‚ã‚Šã€æ‰€å±ã™ã‚‹ä¼æ¥­ãƒ»çµ„ç¹”ã®å…¬å¼ãªè¦‹è§£ã‚’ç¤ºã™ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</div>
      </div>
    </footer>
  </div>
  <script src="/js/lightbox.js" defer></script>
  <script src="/js/toc-highlight.js" defer></script>
  <script src="/js/code-copy.js" defer></script>
  <script src="/js/dark-mode.js" defer></script>
  <script src="/js/mobile-nav.js" defer></script>
  <script src="/js/search.js" defer></script>
  <script src="/js/reading-progress.js" defer></script>
</body>

</html>
