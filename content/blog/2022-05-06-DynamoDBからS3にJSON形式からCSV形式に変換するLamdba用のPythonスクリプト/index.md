---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "DynamoDBã‹ã‚‰kinesis firehoseçµŒç”±ã§S3ã«å‡ºåŠ›æ™‚ã«JSONå½¢å¼ã‹ã‚‰CSVå½¢å¼ã«å¤‰æ›ã™ã‚‹Lamdbaç”¨ã®Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆ"
subtitle: ""
summary: " "
tags: ["AWS"]
categories: ["AWS"]
url: aws-dynamodb-to-s3-csv-transform-python-lamdba
date: 2022-05-06
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your pages folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: fals
---



### ã¯ã˜ã‚ã«

DynamoDBã¸ã®æ›´æ–°ã‚’S3ã«é€£æºã—ãŸã„å ´åˆã¯ä¸‹è¨˜ã®é€šã‚Škinesis firehoseã‚’çµŒç”±ã•ã›ã‚‹ã“ã¨ã§ç°¡å˜ã«ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚ï¼ˆå¾“æ¥ã¯DynamoDB-Lamdba-Kinesisã¨ã™ã‚‹ã“ã¨ãŒå¿…è¦ã ã£ãŸã€‚ï¼‰

> DynamoDBã‹ã‚‰kinesis firehoseçµŒç”±ã§S3ã«å‡ºåŠ› | my opinion is my own ğŸ‘‹ https://zatoima.github.io/aws-dynamodb-to-s3-by-kinesis/

ãŒã€ä¸è¦ãªãƒ¬ã‚³ãƒ¼ãƒ‰ã‚‚ä¸€ç·’ã«å‡ºåŠ›ã•ã‚Œã‚‹ã®ã§å¿…è¦ãªã‚‚ã®ã ã‘ã‚’æŠ½å‡ºã—ãŸã„å ´åˆã¯ã€Kinesis firehoseã®å¤‰æ›æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚Node.jsã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å…¬å¼ã‹ã‚‰ã®æä¾›ã•ã‚Œã¦ã„ã‚‹ãŒã€Pythonã¯ã‚ã¾ã‚Šè¦‹ã¤ã‘ã‚‰ã‚Œãªã‹ã£ãŸã€‚

```json
{"awsRegion":"ap-northeast-1","eventID":"1e84f1ca-438d-4837-a6bd-aa4592be6f8a","eventName":"MODIFY","userIdentity":null,"recordFormat":"application/json","tableName":"cities","dynamodb":{"ApproximateCreationDateTime":1651495525900,"Keys":{"key":{"S":"t0926"}},"NewImage":{"population":{"N":"54148"},"key":{"S":"t0926"},"name":{"S":"ä¸‹é‡"},"date_mod":{"S":"1950-9-7"}},"OldImage":{"population":{"N":"56148"},"key":{"S":"t0926"},"name":{"S":"ä¸‹é‡"},"date_mod":{"S":"1950-9-7"}},"SizeBytes":104},"eventSource":"aws:dynamodb"}
```

### å¿…è¦ãªè¨­å®š

Firehoseå´ã§S3ã«ä¿å­˜æ™‚ã«å¤‰æ›ã—ãŸã„å ´åˆã¯ã€`Transform source records with AWS Lambda`ã‚’`enable`ã«è¨­å®šã—ã¦Lambda Functionã‚’è¨­å®šã™ã‚‹

![image-20220506114740632](image-20220506114740632.png)

### DynamoDBå´ã®ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ 

idã¨datetimeã ã‘ãŒç”¨æ„ã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«

![image-20220506115206214](image-20220506115206214.png)

ä¸‹è¨˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§é©å½“ã«ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ã™ã‚‹ã€‚

```sh
for i in {0..10000}
do
  NOW=`date --iso-8601=seconds`
  aws dynamodb put-item --table-name dynamotest --item "{ \"datetime\": { \"S\": \"${NOW}\" }, \"id\": { \"S\": \"${i}\" } }"
  echo $i
  sleep $(($RANDOM % 5))
done
```

### Lambdaã®Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Kinesis Firehoseå´ã®æ±ºã¾ã‚Šã”ã¨ã«æ²¿ã†å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã€‚

> Amazon Kinesis Data Firehose ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ› - Amazon Kinesis Data Firehose https://docs.aws.amazon.com/ja_jp/firehose/latest/dev/data-transformation.html

> DynamoDB Streamsã¨Kinesis Data Firehoseã‚’ä½¿ã£ãŸã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ETL - Qiita https://qiita.com/kzk-maeda/items/6dd02b9b567a6dd063fd
>
> - Kinesis Data Firehoseã‹ã‚‰é€£æºã•ã‚ŒãŸPayload SizeãŒ6MBã‚’è¶…ãˆã‚‹ã¨ãã¯ã€Payloadã‚’Firehoseã«æˆ»ã™å¿…è¦ãŒã‚ã‚‹
>   - ã•ã‚‰ã«ã€ãã®æˆ»ã™Recordæ•°ãŒ500ã‚’è¶…ãˆã‚‹å ´åˆã¯åˆ†å‰²ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
> - Transformå‡¦ç†ãŒå®Œäº†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’Firehoseã«æˆ»ã™éš›ã€æ‰€å®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã—ãŸãŒã£ãŸå½¢å¼ã§ã€Dataã®å®Ÿæ…‹ã¯base64ã§encordã™ã‚‹å¿…è¦ãŒã‚ã‚‹

ä¸‹è¨˜ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰åˆ¶é™ã‚„Recordæ•°ã¯è€ƒæ…®ã—ã¦ã„ãªã„ã®ã§æ³¨æ„ã€‚æ›´æ–°é‡ãŒå¤šã„DynamoDBã«é–¢ã—ã¦ã¯è¦æ³¨æ„ã‹ã‚‚ã—ã‚Œãªã„ã€‚

```python
import json
import base64

def lambda_handler(event, context):

    results = []
    records = event["records"]
    for record in records:
        record_id = record.get('recordId')
        data = json.loads(base64.b64decode(record.get('data')))
        #print("Raw Data : " + str(data))
        
        id = data['dynamodb']['Keys']['id']['S']
        datetime = data['dynamodb']['Keys']['datetime']['S']

        #print("New Data : " + str(id) + str(datetime))
        return_data = ','.join([id,datetime])  + '\n'
        #print("return_data : "  +  str(return_data))
     
        data = base64.b64encode(return_data.encode())

        results.append({
            "result":"Ok",
            "recordId":record_id,
            "data":data
        })
        
    return {
        "records":results
    }
```

ãªãŠã€DynamoDBå´ã‹ã‚‰æµã‚Œã¦ãã‚‹JSONãƒ‡ãƒ¼ã‚¿ã¯ä¸‹è¨˜ãªã®ã§ã€ã“ã¡ã‚‰ã®JSONãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦å¿…è¦ãªã‚‚ã®ã ã‘ã‚’æŠ½å‡ºã™ã‚‹ã€‚ä»Šå›ã¯ã‚«ãƒ©ãƒ ï¼ˆ`id`ã¨`datetime`ï¼‰ã«å¯¾ã™ã‚‹æŒ¿å…¥æ“ä½œã‚’CSVã§å‡ºåŠ›ã™ã‚‹ã‚ˆã†ã«ã—ãŸã€‚

â€»ã‚µãƒ³ãƒ—ãƒ«ãªã®ã§ã€å®Ÿéš›ã®ã‚«ãƒ©ãƒ ã§ã¯ãªã„ã§ã™

```json
{
  "Records": [
    {
      "eventID": "c4ca4238a0b923820dcc509a6f75849b",
      "eventName": "INSERT",
      "eventVersion": "1.1",
      "eventSource": "aws:dynamodb",
      "awsRegion": "us-east-1",
      "dynamodb": {
        "Keys": {
          "Id": {
            "N": "101"
          }
        },
        "NewImage": {
          "Message": {
            "S": "New item!"
          },
          "Id": {
            "N": "101"
          }
        },
        "ApproximateCreationDateTime": 1428537600,
        "SequenceNumber": "4421584500000000017450439091",
        "SizeBytes": 26,
        "StreamViewType": "NEW_AND_OLD_IMAGES"
      },
      "eventSourceARN": "arn:aws:dynamodb:us-east-1:123456789012:table/ExampleTableWithStream/stream/2015-06-27T00:48:05.899"
    },
    {
      "eventID": "c81e728d9d4c2f636f067f89cc14862c",
      "eventName": "MODIFY",
      "eventVersion": "1.1",
      "eventSource": "aws:dynamodb",
      "awsRegion": "us-east-1",
      "dynamodb": {
        "Keys": {
          "Id": {
            "N": "101"
          }
        },
        "NewImage": {
          "Message": {
            "S": "This item has changed"
          },
          "Id": {
            "N": "101"
          }
        },
        "OldImage": {
          "Message": {
            "S": "New item!"
          },
          "Id": {
            "N": "101"
          }
        },
        "ApproximateCreationDateTime": 1428537600,
        "SequenceNumber": "4421584500000000017450439092",
        "SizeBytes": 59,
        "StreamViewType": "NEW_AND_OLD_IMAGES"
      },
      "eventSourceARN": "arn:aws:dynamodb:us-east-1:123456789012:table/ExampleTableWithStream/stream/2015-06-27T00:48:05.899"
    },
    {
      "eventID": "eccbc87e4b5ce2fe28308fd9f2a7baf3",
      "eventName": "REMOVE",
      "eventVersion": "1.1",
      "eventSource": "aws:dynamodb",
      "awsRegion": "us-east-1",
      "dynamodb": {
        "Keys": {
          "Id": {
            "N": "101"
          }
        },
        "OldImage": {
          "Message": {
            "S": "This item has changed"
          },
          "Id": {
            "N": "101"
          }
        },
        "ApproximateCreationDateTime": 1428537600,
        "SequenceNumber": "4421584500000000017450439093",
        "SizeBytes": 38,
        "StreamViewType": "NEW_AND_OLD_IMAGES"
      },
      "eventSourceARN": "arn:aws:dynamodb:us-east-1:123456789012:table/ExampleTableWithStream/stream/2015-06-27T00:48:05.899"
    }
  ]
}
```

### å¾Œæ—¥è¿½è¨˜â‘ ï¼š

Node.jsã§ã‚ã‚Œã°ä¸‹è¨˜ã‚’å‚è€ƒ

```javascript
'use strict';
console.log('Loading function');

/* Stock Ticker format parser */
const parser = /^\{\"TICKER_SYMBOL\"\:\"[A-Z]+\"\,\"SECTOR\"\:"[A-Z]+\"\,\"CHANGE\"\:[-.0-9]+\,\"PRICE\"\:[-.0-9]+\}/;

exports.handler = (event, context, callback) => {
    let success = 0; // Number of valid entries found
    let failure = 0; // Number of invalid entries found
    let dropped = 0; // Number of dropped entries 

    /* Process the list of records and transform them */
    const output = event.records.map((record) => {

        const entry = (new Buffer(record.data, 'base64')).toString('utf8');
        let match = parser.exec(entry);
        if (match) {
            let parsed_match = JSON.parse(match); 
            var milliseconds = new Date().getTime();
            /* Add timestamp and convert to CSV */
            const result = `${milliseconds},${parsed_match.TICKER_SYMBOL},${parsed_match.SECTOR},${parsed_match.CHANGE},${parsed_match.PRICE}`+"\n";
            const payload = (new Buffer(result, 'utf8')).toString('base64');
            if (parsed_match.SECTOR != 'RETAIL') {
                /* Dropped event, notify and leave the record intact */
                dropped++;
                return {
                    recordId: record.recordId,
                    result: 'Dropped',
                    data: record.data,
                };
            }
            else {
                /* Transformed event */
                success++;  
                return {
                    recordId: record.recordId,
                    result: 'Ok',
                    data: payload,
                };
            }
        }
        else {
            /* Failed event, notify the error and leave the record intact */
            console.log("Failed event : "+ record.data);
            failure++;
            return {
                recordId: record.recordId,
                result: 'ProcessingFailed',
                data: record.data,
            };
        }
        /* This transformation is the "identity" transformation, the data is left intact 
        return {
            recordId: record.recordId,
            result: 'Ok',
            data: record.data,
        } */
    });
    console.log(`Processing completed.  Successful records ${output.length}.`);
    callback(null, { records: output });
};
```

å¾Œæ—¥è¿½è¨˜â‘¡ï¼š

ä¸‹è¨˜ã‚’å…¨é¢çš„ã«æ¡ç”¨ã•ã›ã¦ã„ãŸã ãã€ãã®ã¾ã¾ã§ã¯ã‚¨ãƒ©ãƒ¼ã§å‹•ä½œã—ãªã‹ã£ãŸãŸã‚ã€ä¸€éƒ¨ä¿®æ­£ã—ã¦å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã—ãŸã€‚

> DynamoDB Streamsã¨Kinesis Data Firehoseã‚’ä½¿ã£ãŸã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ETL - Qiita https://qiita.com/kzk-maeda/items/6dd02b9b567a6dd063fd

```python
import json
import boto3
import base64
from datetime import datetime

PAYLOAD_MAX_SIZE = 6000000
MAX_RECORD_COUNT = 500

def transform(data):
    """
    ãƒ‡ãƒ¼ã‚¿å¤‰æ›é–¢æ•°
    """
    data['NewColumn'] = 'New Value'
    # Change Schema
    id = data['dynamodb']['Keys']['id']['S']
    datetime = data['dynamodb']['Keys']['datetime']['S']
    print("New Data : " + str(id) + str(datetime))

    return_data = ','.join([id,datetime])
    print("return_data : "  +  str(return_data))

    return return_data


def proceed_records(records):
    """
    transform each data and yield each record
    """
    for record in records:
        record_id = record.get('recordId')
        data = json.loads(base64.b64decode(record.get('data')))
        print("Raw Data : " + str(data))
        try:
            transformed_data = transform(data)
            result = 'Ok'
        except Exception as e:
            print(e)
            transformed_data = data
            result = 'ProcessingFailed'
        print("New Data : " + str(transformed_data))

        proceeded_data = json.dumps(transformed_data) + '\n'
        proceeded_data = str(transformed_data) + '\n'

        return_record = {
            "recordId": record_id,
            "result": result,
            "data": base64.b64encode(proceeded_data.encode('utf-8'))
        }

        yield return_record


def put_records_to_firehose(streamName, records, client):
    print('Trying to return record to firehose')
    print(f'Item count: {len(records)}')
    print(f'Record: {str(records)}')
    try:
        response = client.put_record_batch(DeliveryStreamName=streamName, Records=records)
    except Exception as e:
        # failedRecords = records
        errMsg = str(e)
        print(errMsg)


def lambda_handler(event, context):
    invocation_id = event.get('invocationId')
    event_records = event.get('records')
    # Transform Data
    records = list(proceed_records(event_records))

    # Check Data
    projected_size = 0 # Responseã‚µã‚¤ã‚ºãŒ6MBã‚’è¶…ãˆãªã„æ§˜åˆ¶å¾¡
    data_by_record_id = {rec['recordId']: _create_reingestion_record(rec) for rec in event['records']}
    total_records_to_be_reingested = 0
    records_to_reingest = []
    put_record_batches = []
    for idx, rec in enumerate(records):
        if rec['result'] != 'Ok':
            continue
        projected_size += len(rec['data']) + len(rec['recordId'])
        if projected_size > PAYLOAD_MAX_SIZE:
            """
            Lambda åŒæœŸå‘¼ã³å‡ºã—ãƒ¢ãƒ¼ãƒ‰ã«ã¯ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ä¸¡æ–¹ã«ã¤ã„ã¦ã€
            ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã« 6 MB ã®åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚
            https://docs.aws.amazon.com/ja_jp/firehose/latest/dev/data-transformation.html
            """
            print(f"Payload size has been exceeded over {PAYLOAD_MAX_SIZE/1000/1000}MB")
            total_records_to_be_reingested += 1
            records_to_reingest.append(
                _get_reingestion_record(data_by_record_id[rec['recordId']])
            )
            records[idx]['result'] = 'Dropped'
            del(records[idx]['data'])

        if len(records_to_reingest) == MAX_RECORD_COUNT:
            """
            Each PutRecordBatch request supports up to 500 records.
            https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecordBatch.html
            """
            print(f'Records count has been exceeded over {MAX_RECORD_COUNT}')
            put_record_batches.append(records_to_reingest)
            records_to_reingest = []

    if len(records_to_reingest) > 0:
        # add the last batch
        put_record_batches.append(records_to_reingest)

    # iterate and call putRecordBatch for each group
    records_reingested_already = 0
    stream_arn = event['deliveryStreamArn']
    region = stream_arn.split(':')[3]
    stream_name = stream_arn.split('/')[1]
    if len(put_record_batches) > 0:
        client = boto3.client('firehose', region_name=region)
        for record_batch in put_record_batches:
            put_records_to_firehose(stream_name, record_batch, client)
            records_reingested_already += len(record_batch)
            print(f'Reingested {records_reingested_already}/{total_records_to_be_reingested} records out of {len(event["records"])}')
    else:
        print('No records to be reingested')


    # Return records to Firehose
    return_records = {
        'records': records
    }
    print(str(return_records))
    return return_records


# Transform method for temporary data
def _create_reingestion_record(original_record):
    return {'data': base64.b64decode(original_record['data'])}

def _get_reingestion_record(re_ingestion_record):
    return {'Data': re_ingestion_record['data']}
```

### å‚è€ƒ

> - Amazon Kinesis Firehose Data Transformation with AWS Lambda | AWS Compute Blog https://aws.amazon.com/jp/blogs/compute/amazon-kinesis-firehose-data-transformation-with-aws-lambda/
> - DynamoDB Streamsã¨Kinesis Data Firehoseã‚’ä½¿ã£ãŸã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ETL - Qiita https://qiita.com/kzk-maeda/items/6dd02b9b567a6dd063fd
> - Amazon DynamoDB ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä½¿ç”¨ã—ã¦ã€é †åºä»˜ã‘ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–“ã§ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹æ–¹æ³• | Amazon Web Services ãƒ–ãƒ­ã‚° https://aws.amazon.com/jp/blogs/news/how-to-perform-ordered-data-replication-between-applications-by-using-amazon-dynamodb-streams/
